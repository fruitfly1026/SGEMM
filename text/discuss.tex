\section{Generality}
\label{sec:generality}

Although this paper examines the methodology as depicted in Figure~\ref{} on Kepler architecture and shows its performance optimizations for SGEMM, we argue that the developed toolchain can be easily extended to other NVIDIA GPUs and the explored optimization strategies is applicable to other floating-point intensive applications.

{\em {\bf Generality of toolchain}}: For a given new GPU architecture, users only need to regenerated disassembly codes with CUDA binary utilities, and then feed them to the solvers, which are portable among different GPU architecture. We have validated the functionalities of the solvers on Kepler, Maxwell and Pascal GPUs. The new assembler can be obtained by modifying the instruction grammar definition decoded by the solvers. In fact, this modification can be automatically generated with an assembler template~\cite{}. 

{\em {\bf Generality of optimizations}}: The optimization strategies include register allocation, memory load/store width and FFMA dual-issue. First, the former two strategies are applicable to other GPU architectures by leveraging the benchmark to detect effective patterns. Only is the third one specific to Kepler architecture. Second, note that the optimizations are microarchitectural specific rather than application specific. In fact, the previous investigations of GEMM optimization on CPU have inspired general performance tuning and compiler optimizations~\cite{}.  For another floating-point intensive algorithm like convolution in deep learning applications, the FFMA throughput can be improved by eliminating register bank conflicts and activating dual issue. Then non-FFMA instructions are carefully scheduled without affecting the FFMA throughput. 


