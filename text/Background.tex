\section{Background}
\label{sec:background}

This section first highlights tunable factors that determine SGEMM performance on the GPU architecture, then introduces some CUDA binary utilities related to SGEMM optimization at assembly level for self-containment. 
% Since we seek to optimize SGEMM at assembly-level, this section introduces some CUDA binary utilities used in our work. 


\subsection{Performance Factors of SGEMM}
\label{sec:sgemm}
%It's well-known that the primary factors of SGEMM performance are the blocking parameters for exploiting data reuse 
%through memory hierarchy. On GPUs, we should consider both shared memory blocking and register blocking. For the 
%purpose of completeness, we describe a blocking algorithm which is similar to that in other 
%literature~\cite{magma,nervana_sgemm_wiki,lai,tan}.
The state of art implementation of SGEMM on GPU~\cite{magma,nervana_sgemm_wiki,lai,tan} uses two level blocking:
shared memory blocking and register blocking to exploit data reuse through memory hierarchy.
%SGEMM is to evaluate the product $C = C + AB$, where $A$, $B$ and $C$ are $m\times k$, $k\times n$ and $m\times n$
The GEMM routines compute a scalar-matrix-matrix product and add the result to
a scalar-matrix product, with general matrices. The operation is defined as $C
= beta*C + alpha*AB$, where $A$, $B$ and $C$ are $m\times k$, $k\times n$ and
$m\times n$ matrices respectively, while alpha and beta are float scalar constants respectively.

Algorithm~\ref{gemm} shows the skeleton of SGEMM blocking algorithm. Task partition is based on the result matrix $C$. 
Each thread block with $t_x*t_y$ threads is responsible to compute a $b_m*b_n$ submatrix in $C$, where $b_m, b_n$ are the 
number of rows and columns of the submatrix $A$ and $B$ respectively. We need to read a $b_m*b_k$ from $A$ and a $b_k*b_n$ sub-matrix from 
$B$ for each thread block, where $b_k$ is the unrolling factor. In this way, $A$, $B$ and $C$ are divided into $M*K$, $K*N$ and 
$M*N$ grids of $b_m*b_k$, $b_k*b_n$ and $b_m*b_n$ blocks, where $M=\Bigl\lfloor \frac{m+b_m-1}{b_m} \Bigr\rfloor$, 
$K=\Bigl\lfloor \frac{k+b_k-1}{b_k} \Bigr\rfloor$, $N=\Bigl\lfloor \frac{n+b_n-1}{b_n} \Bigr\rfloor$.

\begin{algorithm}
  \caption{SGEMM blocking algorithm. {\em smA} and {\em smB} are shared memory variables. {\em rA}, {\em rB} and {\em 
accum} are register variables. $r_x$ and $r_y$ are register blocking sizes}
  \label{gemm}
  \begin{algorithmic}[1]
	\LineComment {The size of a thread block: $t_x*t_y$}
	\LineComment {Registers: accum[$r_x*r_y$], rA[$2*r_x$], rB[$2*r_y$]}
	\State $smA[b_m][b_k] \gets$ a $b_m * b_k$ block of $A$
	\State $smB[b_k][b_n] \gets$ a $b_k * b_n$ block of $B$
	\Do
      \For {{$i \gets 1, b_k$}}
      %\Comment {\color {mygray} {Unrolling for loop}}
      \Comment {{Unrolling for loop}}
      \State {\color {black} {$rA[0...r_x]\gets$ a column of $smA$}}
	\State $rB[0...r_y]\gets$ a row of $smB$
	\State $accum[0...r_x][0...r_y]\gets accum[0...r_x][0...r_y]+rA[0...r_x]*rB[0...r_y]$
	\EndFor
	\State $smA[b_k][b_m]\gets$ a $b_m*b_k$ block of $A$
	\State $smB[b_k][b_n]\gets$ a $b_k*b_n$ block of $B$
	\State \textbf{sync}
	\doWhile {pointer in $B$ is not out of range}
	\State \textbf{merge} $accum[0...r_x][0...r_y]$ with a $b_m*b_n$ block of $C$.
  \end{algorithmic}
\end{algorithm}

The volume of float point computations inside the while loop of Algorithm~\ref{gemm} is $2\times r_x\times r_y \times b_k$ 
in operations, and Table~\ref{tab:dm} estimates data movement volume through registers, shared memory, and global 
memory for each thread inside the while loop.
These parameters demonstrate that SGEMM performance is affected by the hierarchical memory blocking and unrolling 
factors in the inner loops.
The tuning of these factors is conducted in two folds:memory bandwidth and latency. In fact, with the estimations in 
Table~\ref{tab:dm}, it is relatively easy to tune proper parameters to eliminate the bound of memory 
bandwidth~\cite{magma}~\cite{tan}. The difficulty lies in latency, which is closely related to specific 
microarchitectures such as instruction sequence and instruction type. However, the microarchitectural 
optimizations depend on the availability of an assembler and deep understanding of microarchitectural features.

\begin{table}[htbp]
    \caption{Data movement volume of each thread inside the while loop.} %\jled{this table is not referred in this paper.}}
\centering
\scalebox{1.0} {
\begin{tabular}{|c|c|}
\hline
    Data path& Volume(in words)\\
\hline
    global$\Rightarrow$register ({\tt LDG})& $\frac{r_x*\times r_y \times b_k}{b_m} + \frac{r_x\times r_y \times b_k}{b_n}$ \\
\hline
register$\Rightarrow$shared ({\tt STS})& $\frac{r_x*\times r_y \times b_k}{b_m} + \frac{r_x\times r_y \times b_k}{b_n}$ \\
\hline
shared$\Rightarrow$register ({\tt LDS})& $r_x\times b_k + r_y\times b_k$\\
\hline
%    arithemtic({\tt FFMA})& $r_x\times r_y \times b_k$\\
%\hline
\end{tabular}
}
\label{tab:dm}
\end{table}
\subsection{CUDA Binary Utilities}
\label{sec:cuda}
%NVIDIA does not release its GPU assembler to public, they release other tool-chains:ptxas, nvcc.
Though no official GPU assembler is public avaible, NVIDIA provides PTX assembly compiler {\tt PTXAS}, disassembly tools {\tt cuobjdump} and {\tt nvdisasm} which make it possible to build a GPU assembler by third-party.
For instance, both {\tt cuobjdump} and {\tt nvdisasm} disassemble {\em cubin} files to {\em sass} files, which are 
readable assembly codes for examining possible performance issues in GPU programs. For each instruction in the 
executable code sections, the tools list its address in hexadecimal, mnemonics in a string, and encoded number in 
hexadecimal. For example, an {\tt IADD} instruction might be printed as follows: \\\\
$/*0048*/~~~~IADD~~R0,~~R2,~~R0;~~/* 0x4800000000201c03 */$\\\\
Unfortunately, these tools do not enable us to modify the assembly codes directly for tuning performance. What we can 
do is to refine CUDA C codes after examining the read-only assembly codes iteratively. Therefore, an assembler is 
required to manipulate assembly codes. Although NVIDIA does not release its internal assembler and instruction encoding 
format, the disassembled code and the released pseudo assembly references~\cite{ptx2015isa}, provide us clues to crack 
instruction format (in Section~\ref{sec:assembler}).
%NVIDIA provides {\tt PTXAS} to compile PTX assembly to cubin.
%PTX is a low-level parallel thread execution virtual machine and instruction set architecture (ISA)~\cite{}.
%The goals for PTX include the following:
PTX defines a virtual machine and ISA for general purpose parallel thread execution.
%PTX programs are translated at install time to the target hardware instruction set. The
%PTX-to-GPU translator and driver enable NVIDIA GPUs to be used as programmable
%parallel computers. 
The goal of PTX is to provide a stable machine-independent
ISA that spans multiple GPU generations for compilers to target.
Since PTX assembly is machine-independent, it is a kind of pseudo-assembly. There are several disadvantages of it:
First, PTX assembly use pseudo-registers, you can not control register
allocation. Second, as Kepler and Maxwell GPU use control code to do warp
scheduling, but in PTX assembly no control code information.
By using {\tt PTXAS}, we could compile PTX code to Cubin, then use disassemble
tool {\tt cuobjdump} to generate the native assembly with control code. The disassembly is the input of GPU ISA solver.
%PTX exposes the GPU as a data-parallel computing device.
%We can only use CUDA/OpenCL and PTX on NVIDIA GPU, no assembly is supported publicly by NVIDIA company. 
%\jled{introduce instruction and some mentioned opcodes in this paper.}
%A CUDA binary file, also called {\em cubin}, is an ELF-formatted file generated by CUDA compiler ({\tt nvcc}). It can 
%be either embedded into a host executable file or generated separately by using the "{\em -cubin}" option of {\tt 
%nvcc}. To help developers understand {\em cubin} files, CUDA toolkit introduces binary tools and documents on ISAs from 
%GT200 to Maxwell architecture~\cite{gtx980}. These binary tools only provide very limited functionalities for users. 
