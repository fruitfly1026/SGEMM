\section{Background}
\label{sec:background}

This section first highlights tunable factors that determine SGEMM performance on the GPU architecture, then introduces some CUDA binary utilities related to SGEMM optimization at assembly level for self-containment. 
% Since we seek to optimize SGEMM at assembly-level, this section introduces some CUDA binary utilities used in our work. 


\subsection{Performance Factors of SGEMM}
\label{sec:sgemm}
%It's well-known that the primary factors of SGEMM performance are the blocking parameters for exploiting data reuse 
%through memory hierarchy. On GPUs, we should consider both shared memory blocking and register blocking. For the 
%purpose of completeness, we describe a blocking algorithm which is similar to that in other 
%literature~\cite{magma,nervana_sgemm_wiki,lai,tan}.
The state-of-the-art SGEMM implementation on GPUs~\cite{magma,nervana_sgemm_wiki,lai,tan} uses two-level blocking,
register and shared memory blocking, to exploit the data reuse through GPU memory hierarchy.
%SGEMM is to evaluate the product $C = C + AB$, where $A$, $B$ and $C$ are $m\times k$, $k\times n$ and $m\times n$
The SGEMM routine computes a scalar-matrix-matrix product and updates the result plus
a scalar-matrix product on single floating-point precision general matrices and scalars. 
The operation is defined as $C
= beta*C + alpha*AB$, where $A$, $B$ and $C$ are $m\times k$, $k\times n$ and
$m\times n$ matrices respectively, while $alpha$ and $beta$ are scalar constants.

Algorithm~\ref{gemm} shows the skeleton of the blocked SGEMM algorithm. 
Task partition is based on the result matrix $C$. 
Every $t_x \times t_y$ thread block is responsible to compute a $b_m \times b_n$ matrix block of $C$ by reading an $A$'s $b_m \times b_k$ block and a $B$'s $b_k \times b_n$ block, where $b_k$ is the unrolling factor.
% where $b_m$ is also the number of rows of $A$'s matrix block, $b_n$ is also the number of columns of $B$'s matrix block. 
% Each thread block reads a $b_m \times b_k$ from $A$ and a $b_k*b_n$ sub-matrix from $B$ for each thread block, where $b_k$ is the unrolling factor. 
Therefore, $A$, $B$, and $C$ are divided into $M*K$, $K*N$ and 
$M*N$ grids with $b_m \times b_k$, $b_k \times b_n$, and $b_m \times b_n$ blocks/grid, where $M=\Bigl\lfloor \frac{m+b_m-1}{b_m} \Bigr\rfloor$, 
$K=\Bigl\lfloor \frac{k+b_k-1}{b_k} \Bigr\rfloor$, and $N=\Bigl\lfloor \frac{n+b_n-1}{b_n} \Bigr\rfloor$.

\begin{algorithm}
  \caption{Blocked SGEMM algorithm of a $t_x \times t_y$ thread block. $b_m, b_n, b_k$ are shared memory blocking sizes, $r_x$ and $r_y$ are register blocking sizes.\jli{wait for modification}}
  \label{gemm}
  {\footnotesize
  \begin{algorithmic}[1]
	\LineComment {Shared memory variables: $sm_A$ and $sm_B$}
	\LineComment {Registers variables: $accum[r_x \times r_y]$, $r_A[r_x]$, $r_B[r_y]$, $r'_A[r_x]$, $r'_B[r_y]$.}
	\State $sm_A \gets$ a $b_m \times b_k$ block of $A$
	\State $sm_B \gets$ a $b_k \times b_n$ block of $B$
	\State \textbf{sync}
	\Do
    \For {$i \gets 0$ to $b_k-1$ step $2$}
    \Comment {loop unrolling}
    \LineComment {Thread-local computation} 
    \State $r_A \gets sm_A[0, \dots, r_x-1][i]$
		\State $r_B \gets sm_B[i][0, \dots, r_y-1]$
		\State $accum \gets accum + r_A \circ r_B$
		\Comment {$\circ$: outer-product}
		\State $r'_A \gets sm_A[0, \dots, r_x-1][i+1]$
		\State $r'_B \gets sm_B[i+1][0, \dots, r_y-1]$
		\State $accum \gets accum + r'_A \circ r'_B$
	\EndFor
	\State $sm_A \gets$ another $b_m \times b_k$ block of $A$
	\State $sm_B \gets$ another $b_k \times b_n$ block of $B$
	\State \textbf{sync}
	\doWhile {One more valid $b_k \times b_n$ block of $B$ exists.\jli{further polish}}
	\State \textbf{merge} $accum$ with a $b_m \times b_n$ block of $C$.
  \end{algorithmic}
  }
\end{algorithm}

% \begin{algorithm}
%   \caption{SGEMM blocking algorithm. {\em smA} and {\em smB} are shared memory variables. {\em rA}, {\em rB} and {\em 
% accum} are register variables. $r_x$ and $r_y$ are register blocking sizes}
%   \label{gemm}
%   \begin{algorithmic}[1]
% 	\LineComment {The size of a thread block: $t_x*t_y$}
% 	\LineComment {Registers: accum[$r_x*r_y$], rA[$2*r_x$], rB[$2*r_y$]}
% 	\State $smA[b_m][b_k] \gets$ a $b_m * b_k$ block of $A$
% 	\State $smB[b_k][b_n] \gets$ a $b_k * b_n$ block of $B$
% 	\Do
%       \For {{$i \gets 1, b_k$}}
%       %\Comment {\color {mygray} {Unrolling for loop}}
%       \Comment {{Unrolling for loop}}
%       \State {\color {black} {$rA[0...r_x]\gets$ a column of $smA$}}
% 	\State $rB[0...r_y]\gets$ a row of $smB$
% 	\State $accum[0...r_x][0...r_y]\gets accum[0...r_x][0...r_y]+rA[0...r_x]*rB[0...r_y]$
% 	\EndFor
% 	\State $smA[b_k][b_m]\gets$ a $b_m*b_k$ block of $A$
% 	\State $smB[b_k][b_n]\gets$ a $b_k*b_n$ block of $B$
% 	\State \textbf{sync}
% 	\doWhile {pointer in $B$ is not out of range}
% 	\State \textbf{merge} $accum[0...r_x][0...r_y]$ with a $b_m*b_n$ block of $C$.
%   \end{algorithmic}
% \end{algorithm}

Algorithm~\ref{gemm} has $2 r_x r_y b_k$ floating-point operations (flops) of each thread for one iteration of the while loop.
Table~\ref{tab:dm} estimates data movement volume through registers, shared memory, and global 
memory correspondingly.
These parameters demonstrate that SGEMM performance is determined by the hierarchical memory blocking and the unrolling factor of the inner for loop.
Tuning of these factors relies on GPU memory bandwidth and latency. 
According to the previous work~\cite{magma,tan}, tuning proper parameters to overcome memory bandwidth limits is relatively easy with the estimates in table~\ref{tab:dm}.
Performance tuning for memory latency is much more difficult, since latency is closely coupled to a particular microarchitecture's features such as the instruction sequence and instruction types, which depends on a GPU toolchain and a assembler to identify. 


\begin{table}[htbp]
    \caption{Data movement volume of each thread for one while loop iteration.} %\jled{this table is not referred in this paper.}}
\centering
\scalebox{1.0} {
\begin{tabular}{|c|c|}
\hline
    Data path& Volume (in words)\\
\hline
    global$\Rightarrow$register ({\tt LDG})& $\frac{r_x \times r_y \times b_k}{b_m} + \frac{r_x\times r_y \times b_k}{b_n}$ \\
\hline
register$\Rightarrow$shared ({\tt STS})& $\frac{r_x \times r_y \times b_k}{b_m} + \frac{r_x\times r_y \times b_k}{b_n}$ \\
\hline
shared$\Rightarrow$register ({\tt LDS})& $r_x\times b_k + r_y\times b_k$\\
\hline
%    arithemtic({\tt FFMA})& $r_x\times r_y \times b_k$\\
%\hline
\end{tabular}
}
\label{tab:dm}
\end{table}


\subsection{CUDA Binary Utilities}
\label{sec:cuda}

%NVIDIA does not release its GPU assembler to public, they release other tool-chains:ptxas, nvcc.
To our best knowledge, no official GPU assembler is publicly available.
NVIDIA provides PTX assembly compiler {\tt PTXAS}, disassembly tools {\tt cuobjdump}, and {\tt nvdisasm}, which makes it possible to build a GPU assembler by a third party.
Both {\tt cuobjdump} and {\tt nvdisasm} disassemble {\em cubin} files to {\em sass} files which are human-readable assembly codes for examining potential performance issues in CUDA programs. 
For every instruction in executable code sections, these tools list its address in a hexadecimal, the mnemonics in a string, and the encoded number in another hexadecimal. 
For example, an {\tt IADD} instruction may be recorded as follows: \\\\
{\small
$/*0048*/~~~~IADD~~R0,~~R2,~~R0;~~/* 0x4800000000201c03 */$\\
}

Unfortunately, programmers cannot use these tools to directly modify the read-only {\em sass} assembly codes for performance tuning, but only refine CUDA codes and then re-examine the new generated assembly codes iteratively.
Therefore, an assembler is desiderated to manipulate assembly codes. 
Though NVIDIA hasn't released the internal assembler and instruction encoding format, the disassembled codes in {\em sass} files and the public pseudo assembly references~\cite{ptx2015isa} provide clues to crack its instruction format and then build our own assembler (section~\ref{sec:assembler}).
%NVIDIA provides {\tt PTXAS} to compile PTX assembly to cubin.
%PTX is a low-level parallel thread execution virtual machine and instruction set architecture (ISA)~\cite{}.
%The goals for PTX include the following:


Though NVIDIA provides its assembly language PTX and its compiler {\tt PTXAS}, PTX targets a stable machine-independent ISA that could span multiple GPU generations.
Its machine-independent property is convenient for compilers design but also brings several drawbacks:
First, PTX use pseudo-registers, programmers cannot control its register allocation. 
Second, PTX has no control code information which is used in Kepler and Maxwell GPUs to do warp scheduling.
Therefore, even for PTX codes, we also use {\tt PTXAS} to compile them to \emph{cubin} and then use disassemble tool {\tt cuobjdump} to generate their native assembly codes. 
This paper uses disassembled codes in {\em sass} files as the input of the GPU ISA encoding solver.
% , it is a kind of pseudo-assembly. 
 % for compilers to target.
% PTX defines a virtual machine and ISA for general purpose parallel thread execution.
%PTX programs are translated at install time to the target hardware instruction set. The
%PTX-to-GPU translator and driver enable NVIDIA GPUs to be used as programmable
%parallel computers. 
%PTX exposes the GPU as a data-parallel computing device.
%We can only use CUDA/OpenCL and PTX on NVIDIA GPU, no assembly is supported publicly by NVIDIA company. 
%\jled{introduce instruction and some mentioned opcodes in this paper.}
%A CUDA binary file, also called {\em cubin}, is an ELF-formatted file generated by CUDA compiler ({\tt nvcc}). It can 
%be either embedded into a host executable file or generated separately by using the "{\em -cubin}" option of {\tt 
%nvcc}. To help developers understand {\em cubin} files, CUDA toolkit introduces binary tools and documents on ISAs from 
%GT200 to Maxwell architecture~\cite{gtx980}. These binary tools only provide very limited functionalities for users. 
