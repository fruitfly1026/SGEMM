\section{Introduction}
Single Precision General Matrix Multiply (SGEMM) performs a multiplication of two single-precision matrices and is one 
of the basic routines in BLAS library~\cite{blas}. It has been extensively used in many scientific and engineering 
computing applications. Recently, SGEMM has drawn increasing efforts on performance tuning since it is the performance 
critical kernel in deep learning applications~\cite{chetlur2014cudnn,nervana_sgemm_wiki}.
\jled{Specify the importance of SGEMM in deep learning, and why SGEMM not DGEMM etc.}


As GPU provides higher peak FLOPS over CPU contemporarily, people tend to adopt GPUs to accelerate
floating-point intensive computations. In fact, SGEMM's performance highly relies on low-level microarchitecture 
features. Hardware
vendors provide BLAS libraries tuned on their own processors, e.g. MKL/ACML~\cite{intel2007intel,amd2014} for multicore 
x86 CPUs and CUBLAS/CLMath~\cite{nvidia2008cublas, clmath}for
GPUs. However, we always witness improvement from third-party implementations over these vendors' libraries. For
multicore CPUs, based on hand-tuned assembly codes, OpenBLAS~\cite{xianyi2012openblas} achieves the best performance in 
most cases.
Although there is a lack of an OpenBLAS-like library on GPUs, several ongoing efforts~\cite{tan,lai,nervana_sgemm_wiki,
chien, volkov} achieve better performance than CUBLAS for either SGEMM or DGEMM by tuning assembly codes. However, 
these scattered works remain two issues to be addressed to accomplish the microarchitecture based performance tuning on 
each generation of GPUs.

\begin{itemize}
\item {\em There is a lack of a toolchain to identify GPU microarchitecture features and guide performance tuning.}
    Unlike general-purpose CPU community where a series of toolchains are available to tune performance in a bare metal
        way, only the abstract CUDA model is encouraged. A major reason is the significant changes of GPU architecture
        between each generation. For example, graphic features, register bank distribution, and floating-point 
instructions dual
        issue of the recent ISA are totally different from the previous generations~\cite{fermi}. Fortunately, people 
have made
        some initial progress on performance tuning tools due to the pursuit of extreme performance, including
        benchmarking~\cite{mei, volkov, wong} and disassembler~\cite{asfermi,bernstein2012usable,decuda,maxas} on a 
specific GPU architecture. We attempt to develop a methodology that provides a systematic way to identify 
microarchitecture by decoding instruction formats automatically, generating ISA-compatible executable codes and 
benchmarking.
\item {\em There is a lack of a comprehensive understanding of SGEMM's performance in terms of low-level GPU 
microarchitecture.} Most SGEMM analyses are circumscribed at the levels of either CUDA or PTX because there is short of 
bare-metal tools on GPUs. Unfortunately, these analyses cannot directly diagnose either compiler deficiency or hardware 
defect. In fact, by observing the disassembled code of SGEMM in CUDA, we find that the generated control code is very 
inefficient in exploiting {\tt FFMA} dual issue (See Figure~\ref{fig:assemblycode} in section~\ref{sec:optimization}). 
It indicates that NVCC generated code does not completely utilize $192$ cores on an SM (streaming processor), leaving 
them idle at most of the time. A side effect of the limitation leads to a bias estimation of performance bound. We 
present a thorough analysis of performance which goes through the whole architecture hierarchy, including instruction 
throughput, register allocation, and shared/global memory transactions. The understanding of microarchitecture helps us 
build a robust performance model of SGEMM.
\end{itemize}

In this paper, we crack instruction encoding by performing a tough reverse engineering work, and then build an 
assembler to generate CUDA binary files. Because of the compatible grammar of CUDA {\tt 
cuobjdump}~\cite{cubin2015util}, we use CUDA toolchains~\cite{nvcc} to compile CUDA codes to a {\em cubin} file and 
then disassemble it to generate assembly codes. This approach supports users to optimize any code segment on the base 
of generated codes instead of coding from scratch. By using this assembler, we design a set of microbenchmarks that 
demystify lots of GPU microarchitecture details such as instruction issue, warp schedule, register bank distribution, 
and control code, which are helpful to understand and optimize the performance of SGEMM. We apply a collection of 
optimizations to improve SGEMM performance incrementally. More specifically, we make the following contributions:
\begin{itemize}
\item We propose a reverse engineering approach to crack the instruction encoding of GPU architecture, especially for 
control codes that orchestrate instruction scheduling. An assembler is developed to directly tune the assembly codes 
generated by CUDA compiler.
\item We design a bunch of benchmarks to reveal microarchitectural features which are undocumented by NVIDIA. They are 
necessary to understand and tune the performance of GPU programs.
\item We implement the highest performance SGEMM routines on NVIDIA Kepler by applying the demystified 
microarchitecture-level optimizations. The optimized SGEMM achieves the maximal floating-point efficiency of 88\%, 
which is 17\% higher than CUBLAS.
\end{itemize}

Although this work demonstrates the effectiveness on NVIDIA Kepler architecture, the approach is general-purpose for 
other NVDIA GPU architectures by minor adjustments of instruction solver and benchmarking. To the best of our 
knowledge, it is the first time to provide a detailed description of how to crack the instruction encoding for NVIDIA 
GPUs~\footnote{We'll open the sourcecodes to public in GitHub after the double-blind review.}. The experience of 
exploring bare-metal optimizations is valuable to compiler developement and performance tuning.

The rest of this paper is organized as follows. Section~\ref{sec:background} introduces the CUDA binary utilities and a 
blocking SGEMM algorithm. Section~\ref{sec:assembler} presents the instruction solver algorithms and microbenchmarking 
insights. Section~\ref{sec:optimization} applies a series of microarchitectural optimizations to SGEMM. We report 
experimental results on section~\ref{sec:experiment}. Section~\ref{sec:related} summarizes the related work. Finally, 
section~\ref{sec:conclusion} concludes this work. 
