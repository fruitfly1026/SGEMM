\section{Introduction}
Single Precision General Matrix Multiply (SGEMM) performs a multiplication of two single-precision matrices and is one o
f the basic routines in BLAS library~\cite{blas}. It has been extensively used in many scientific and engineering comput
ing applications. Recently, SGEMM has drawn increasing efforts on performance tuning since it is the performance critica
l kernel in deep learning applications~\cite{chetlur2014cudnn,nervana_sgemm_wiki}.

As GPU provides higher peak FLOPS over CPU contemporarily, people tend to adopt GPUs to accelerate such a floating-point
 intensive computation. In fact, SGEMM's performance highly relies on low-level microarchitecture features. Hardware
vendors provide BLAS libraries tuned on their own processors, e.g. MKL/ACML~\cite{intel2007intel,amd2014} for multicore 
x86 CPUs and CUBLAS/CLMath~\cite{nvidia2008cublas, clmath}for
GPUs. However, we always witness improvement from third-party implementations over these vendors' libraries. For
multicore CPUs, based on hand-tuned assembly codes, OpenBLAS~\cite{xianyi2012openblas} achieves the best performance in 
most cases.
Although there is a lack of an OpenBLAS-like library on GPUs, several ongoing efforts~\cite{tan,lai,nervana_sgemm_wiki,
chien, volkov} achieve better performance than CUBLAS for either SGEMM or DGEMM by tuning assembly codes. However, these
 scattered works remain two issues to be addressed to accomplish the microarchitecture based performance tuning on each 
generation of GPUs.

\begin{itemize}
\item {\em There is a lack of a toolchain to identify GPU microarchitecture features and guide performance tuning.}
    Unlike general-purpose CPU community where a series of toolchains are available to tune performance in a bare metal
        way, only the abstract CUDA model is encouraged. A major reason is the significant changes of GPU architecture
        between each generation. For example, graphic features, register bank conflicts, and floating-point instructions
 dual
        issue of the recent ISA are totally different from the previous generations~\cite{fermi}. Fortunately, people ha
ve made
        some initial progress on performance tuning tools due to the pursuit of extreme performance, including
        benchmarking~\cite{mei, volkov, wong} and disassembler~\cite{asfermi,bernstein2012usable,decuda,maxas} on a spec
ific GPU architecture. We attempt to develop a methodology that provides a systematic way to identify microarchitecture 
by decoding instruction formats automatically, generating ISA-compatible executable codes and benchmarking.
\item {\em There is a lack of a comprehensive understanding of SGEMM's performance in terms of low-level GPU microarchit
ecture.} Most SGEMM analyses are circumscribed at the levels of either CUDA or PTX because there is short of bare-metal 
tools on GPUs. Unfortunately, these analyses cannot directly diagnose either compiler deficiency or hardware defect. In 
fact, by observing the disassembled code of SGEMM in CUDA, we find that the generated control code is very inefficient i
n exploiting {\tt FFMA} dual issue (See Figure~\ref{fig:assemblycode} in section~\ref{sec:optimization}). It indicates t
hat NVCC generated code does not completely utilize $192$ cores on an SM (streaming processor), leaving them idle at mos
t of the time. A side effect of the limitation leads to a bias estimation of performance bound. We present a thorough an
alysis of performance which goes through the whole architecture hierarchy, including instruction throughput, register al
location, and shared/global memory transactions. The understanding of microarchitecture helps us build a robust performa
nce model of SGEMM.
\end{itemize}

In this paper, we crack instruction encoding by performing a tough reverse engineering work, and then build an assembler
 to generate CUDA binary files. Because of the compatible grammar of CUDA {\tt cuobjdump}~\cite{cubin2015util}, we use C
UDA toolchains~\cite{nvcc} to compile CUDA codes to a {\em cubin} file and then disassemble it to generate assembly code
s. This approach supports users to optimize any code segment on the base of generated codes instead of coding from scrat
ch. By using this assembler, we design a set of microbenchmarks that demystify lots of GPU microarchitecture details suc
h as instruction issue, warp schedule, register bank distribution, and control code, which are helpful to understand and
 optimize the performance of SGEMM. We apply a collection of optimizations to improve SGEMM performance incrementally. M
ore specifically, we make the following contributions:
\begin{itemize}
\item We propose a reverse engineering approach to crack the instruction encoding of GPU architecture, especially for co
ntrol codes that orchestrate instruction scheduling. An assembler is developed to directly tune the assembly codes gener
ated by CUDA compiler.
\item We design a bunch of benchmarks to reveal microarchitectural features which are undocumented by NVIDIA. They are n
ecessary to understand and tune the performance of GPU programs.
\item We implement the highest performance SGEMM routines on NVIDIA Kepler by applying the demystified microarchitecture
-level optimizations. The optimized SGEMM achieves the maximal floating-point efficiency of 88\%, which is 17\% higher t
han CUBLAS.
\end{itemize}

Although this work demonstrates the effectiveness on NVIDIA Kepler architecture, the approach is general-purpose for oth
er NVDIA GPU architectures by minor adjustments of instruction solver and benchmarking. To the best of our knowledge, it
 is the first time to provide a detailed description of how to crack the instruction encoding for NVIDIA GPUs~\footnote{
We'll open the sourcecodes to public in GitHub after the double-blind review.}. The experience of exploring bare-metal o
ptimizations is valuable to compiler developement and performance tuning.

The rest of this paper is organized as follows. Section~\ref{sec:background} introduces the CUDA binary utilities and a 
blocking SGEMM algorithm. Section~\ref{sec:assembler} presents the instruction solver algorithms and microbenchmarking i
nsights. Section~\ref{sec:optimization} applies a series of microarchitectural optimizations to SGEMM. We report experim
ental results on section~\ref{sec:experiment}. Section~\ref{sec:related} summarizes the related work. Finally, section~\
ref{sec:conclusion} concludes this work. 
