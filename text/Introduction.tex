
\section{Introduction}
\label{sec:intro}

As GPUs provide higher peak floating-point performance and memory bandwidth than CPUs contemporarily, researchers tend to adopt GPUs to accelerate compute-intensive programs. 
%In fact, SGEMM's performance highly relies on low-level microarchitecture features. 
Hardware vendors provide performance-critical libraries specifically tuned for their own processors, such as Intel MKL~\cite{intel2007intel} and AMD ACML~\cite{amd2014} for multicore x86 CPUs and NVIDIA cuBLAS~\cite{intel2007intel} and AMD clMath~\cite{clmath} for GPUs. 
However, we always witness performance improvement from third-party implementations over these vendor libraries. 
For example, OpenBLAS~\cite{wang2013augem}, based on its hand-tuned assembly codes, achieves the better performance than AMD's ACML and Intel's MLK on Intel's Sandybridge and AMD's Piledriver processors. %in most cases on a multicore x86 CPU.
Although an OpenBLAS-like library is absent from GPUs, several % ongoing 
efforts~\cite{tan,lai,nervana_sgemm_wiki,
chien, volkov} have achieved better performance than cuBLAS's GEMM by tuning assembly codes on NVIDIA GPUs. 
However, % accomplish the microarchitecture based performance tuning on every GPU generation
the disperse work leaves two issues to be addressed for diligently tuning the performance on the microarchitecture of every GPU generation. Since Single-precision General Matrix Multiply (SGEMM) is extensively used in many scientific \& engineering computing applications and deep learning ones as well. Throughout this paper we use SGEMM as a running example for simplicity of presentation.

\begin{itemize}
\item {\em A lack of a toolchain to identify GPU microarchitecture features and guide performance tuning.}
  Unlike the general-purpose CPU community where a series of toolchains are available to tune performance in a bare-metal
  way, only the abstract CUDA model is encouraged in the GPU community. 
  NVIDIA engineers hand tune their supported % cuBLAS routines and other vendor
  libraries in GPU assembly language, this leaves other unsupported algorithms hard to be diligently optimized.
  %The main reason is the significant changes of GPU architecture
  %among generations. For example, graphic features, register bank distribution, and floating-point instructions dual
  %issue of the recent ISAs are totally different from the previous generations~\cite{fermi}. 
  Fortunately, researchers have made some initial progress on performance tuning tools, including benchmarking~\cite{mei, volkov, wong} and designing assemblers on some particular GPU architectures~\cite{asfermi,bernstein2012usable,decuda,maxas}. %\jli{further check}.
% to pursue extreme performance. 
In this paper, we develop a methodology to systematically identify 
microarchitecture by automatically decoding instruction formats, building GPU assembler, and benchmarking.

\item {\em A lack of comprehensive understanding of the computational kernels performance behavior in terms of the low-level GPU microarchitecture.} 
Most SGEMM analyses are circumscribed at CUDA or PTX level due to the short of GPU bare-metal tools. 
Therefore, these studies cannot directly diagnose compiler deficiency or hardware 
defects. 
By observing the disassembled codes of CUDA SGEMM, we find that the CUDA
compiler (NVCC) generated control codes are deficient in exploiting
FP32 Fused Multiply Add ({\tt FFMA}) dual issue feature(details in
section~\ref{sec:ffma-dual}). 
This indicates that the binary codes generated by NVCC cannot utilize GPU cores efficiently. %but leave some of them idle most of the time. 
% completely utilize all cores in a streaming processor (SM), 
A side effect of this compiler deficiency leads to a bias estimation of the performance bound. 
This paper presents a thorough performance analysis which goes through the whole architecture hierarchy, including instruction 
throughput, register allocation, and shared/global memory transfers. 
The identified microarchitecture features help us build robust performance models of SGEMM and other programs.
\end{itemize}


In this paper, we propose a GPU ISA encoding solver to automatically crack GPU ISA encodings by feeding GPU disassembly,
then we build a GPU assembler to generate binaries from hand-optimized assembly codes. %\jli{further check}%CUDA binary files. 
%Taking the advantage of the compatible grammar of CUDA {\tt cuobjdump}~\cite{cubin2015util} with NVCC~\cite{nvcc}, we use NVCC to compile CUDA codes to a {\em cubin} file and then disassemble it to assembly codes. 
Due to the compatible syntax with disassembly generated by {\tt cuobjdump}~\cite{cubin2015util}, we use NVCC
to compile CUDA codes to a cubin file and then disassemble it to generate assembly codes.
This approach allows users to optimize any code segment based on the generated assembly codes instead of coding from scratch. 
With this assembler, a microbenchmark suite is designed to 
demystify plenty of GPU microarchitecture features such as instruction
throughput, warp scheduling, and register bank distribution, which help to
understand and optimize the performance of a computational program. 
We apply optimization methods corresponding to GPU microarchitecture features to improve SGEMM performance incrementally. Besides, we also show the generality of our methodology by optimizing kernels in convolutional networks.
More specifically, we make the following contributions:

\begin{itemize}
\item We propose a GPU ISA encoding solver to automatically crack ISA encodings
     of diverse GPU microarchitectures by feeding disassembly codes.
%instruction encoding of GPU architecture.
A Kepler GPU assembler is developed to directly tune the assembly codes generated by CUDA compiler.
\item We design a microbenchmark suite to explore undocumented
microarchitecture features of NVIDIA GPUs, such as control codes regulating
{\tt FFMA} instruction dual issue and register bank indices influence on
instruction throughput, which are necessary to understand and tune GPU
programs.
\item We implement both SGEMM routine and convolution kernels on a NVIDIA Kepler GPU by applying the demystified microarchitecture-level optimizations. 
The optimized SGEMM achieves up to 88\% of the machine's peak floating-point performance, which is 15\% higher than cuBLAS7.0. The optimized convolution networks gains $39\%$-$62\%$ performance improvement compared to cuDNN4.0.
\end{itemize}

%Albeit Kepler is not the latest generation GPU, the {\tt FFMA} dual-issue feature will not be
%outdated. During our exploring of the method to fully exploit the arithmetic
%dual-issue feature of Kepler, we find that NVCC can not generate dual issue code efficiently even for computation
%intensive SGEMM which prevents dual issue feature further development in the succeeding GPUs. The Dual-issue feature would return in future
%GPU design when scalability of thread level parallelism and frequency of GPU are too hard to improve.
Although this work demonstrates the effectiveness of our methodology on a NVIDIA Kepler GPU, the methodology is general-purpose for 
other NVIDIA GPU architectures under minor adjustments of instruction solver, assembler and benchmarking.
% To the best of our knowledge, it is the first time to provide a detailed description of how to crack the instruction encoding for NVIDIA GPUs automatically.
%To the best of our knowledge, it is the first time to thoroughly describe the automatic cracking of the instruction encodings of NVIDIA GPUs
The experience of 
exploring bare-metal optimizations is helpful to compiler development and performance tuning\footnote{\scriptsize{Code:https://github.com/PAA-NCIC/PPoPP2017\_artifact}}. 

The rest of this paper is organized as follows. Section~\ref{sec:background} introduces 
a general blocking SGEMM algorithm and the CUDA binary utilities.
Section~\ref{sec:assembler} presents the instruction solver algorithms and
microbenchmarking 
insights. Section~\ref{sec:optimization} applies a series of microarchitectural optimizations to SGEMM. We report 
experimental results in Section~\ref{sec:experiment}.
Section~\ref{sec:generality} discusses the generality and portability of our methodology for different GPUs and diverse algorithms.
Section~\ref{sec:related} summarizes the related work. Finally, 
Section~\ref{sec:conclusion} concludes this work. 
