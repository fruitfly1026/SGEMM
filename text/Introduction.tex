\section{Introduction}
Single Precision General Matrix Multiply (SGEMM) performs a multiplication of two single-precision matrices and is one of the basic routines in BLAS library~\cite{blas}. It has been extensively used in many scientific and engineering computing applications. Recently, SGEMM has drawn increasing efforts on performance tuning since it is the performance critical kernel in deep learning applications~\cite{chetlur2014cudnn,nervana_sgemm_wiki}.

As GPU provides higher peak FLOPS over CPU contemporarily, people tend to adopt GPUs to accelerate such a floating-point intensive computation. In fact, SGEMM's performance highly relies on low-level microarchitecture features. Hardware
vendors provide BLAS libraries tuned on their own processors, e.g. MKL/ACML~\cite{intel2007intel,amd2014} for multicore x86 CPUs and CUBLAS/CLMath~\cite{nvidia2008cublas, clmath}for
GPUs. However, we always witness improvement from third-party implementations over these vendors' libraries. For
multicore CPUs, based on hand-tuned assembly codes, OpenBLAS~\cite{xianyi2012openblas} achieves the best performance in most cases.
Although there is a lack of an OpenBLAS-like library on GPUs, several ongoing efforts~\cite{tan,lai,nervana_sgemm_wiki,
chien, volkov} achieve better performance than CUBLAS for either SGEMM or DGEMM by tuning assembly codes. However, these scattered works remain two issues to be addressed to accomplish the microarchitecture based performance tuning on each generation of GPUs.

\begin{itemize}
\item {\em There is a lack of a toolchain to identify GPU microarchitecture features and guide performance tuning.}
    Unlike general-purpose CPU community where a series of toolchains are available to tune performance in a bare metal
        way, only the abstract CUDA model is encouraged. A major reason is the significant changes of GPU architecture
        between each generation. For example, graphic features, bank conflicts, and floating-point instructions dual
        issue of the recent ISA is totally different from the previous first generation~\cite{fermi}. Fortunately, people have made
        some initial progress on performance tuning tools due to the pursuit of extreme performance, including
        benchmarking~\cite{mei, volkov, wong} and disassembler~\cite{asfermi,bernstein2012usable,decuda,maxas} on a specific GPU architecture. We attempt to develop a methodology that provides a systematic way to identify microarchitecture by benchmarking, decode instruction formats automatically, and generate ISA-compatible executable codes.
\item {\em There is a lack of a comprehensive understanding of SGEMM's performance in terms of low-level GPU microarchitecture.} Most SGEMM analyses are circumscribed at the levels of either CUDA or PTX because there is short of bare-metal tools on GPUs. Unfortunately, these analyses cannot directly diagnose either compiler deficiency or hardware defect. In fact, by observing the disassembled code of SGEMM in CUDA, we find that the generated control code is very inefficient in exploiting {\tt FFMA} dual issue. Thus, it indicates that NVCC generated code does not completely utilize $192$ cores on an SM (streaming processor), leaving them idle at most of the time. A side effect of the limitation leads to a bias estimation of performance bound. We present a thorough analysis of performance which goes through the whole architecture hierarchy, including instruction throughput, register allocation, and shared/global memory transactions. The understanding of microarchitecture helps us build a robust performance model of SGEMM.
\end{itemize}

In this paper, we crack instruction encoding by performing a tough reverse engineering work and build an assembler to generate CUDA binary file for ISA-compatible assembly codes. Because of the compatible grammar of CUDA {\tt cuobjdump}~\cite{cubin2015util}, we use CUDA toolchains~\cite{nvcc} to compile CUDA codes to a {\em cubin} file and then disassemble it to generate assembly codes. This approach supports users to optimize any code segment on the base of generated codes instead of coding from scratch. By using this assembler, we design a set of microbenchmarks that demystify lots of GPU microarchitecture details such as instruction issue, warp schedule, register bank distribution, and control code, which are helpful to understand and optimize the performance of SGEMM. We apply a collection of optimizations to improve SGEMM performance incrementally. More specifically, we make the following contributions:
\begin{itemize}
\item We propose a reverse engineering approach to crack the instruction encoding of GPU architecture, especially for control codes that orchestrate instruction scheduling. An assembler is developed to directly tune the assembly codes generated by CUDA compiler.
\item We design a bunch of benchmarks to reveal microarchitectural features which are undocumented by NVIDIA. They are necessary to understand and tune the :performance of GPU programs.
\item We implement the highest performance SGEMM routines on NVIDIA Kepler by applying the demystified microarchitecture-level optimizations. The achieved maximal floating-point efficiency is xx and xx on two GPUs, respectively.
\end{itemize}

Though this work only demonstrates the effectiveness on NVIDIA Kepler architecture~\cite{gk110}, the approach is general for other NVDIA GPU architectures by minor adjustments of instruction solver and benchmarks. Then, the demystified optimizations could be applied to tune performance on other NVIDIA GPU architectures. The rest of this paper is organized as follows. %TODO:finish organization, adjust the second last sentence
