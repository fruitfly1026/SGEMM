\section{Introduction}
Single-precision General Matrix Multiply (SGEMM), as one 
of the basic routines in a BLAS library~\cite{blas,intel2007intel,amd2014}, performs a multiplication of two single-precision matrices. 
It has been extensively used in many scientific and engineering 
computing applications. 
Recently, increasing efforts have been made to tune the performance of SGEMM, because of its critical performance effect on deep learning applications~\cite{chetlur2014cudnn,nervana_sgemm_wiki}.
% as it is a performance critical kernel in
%In convolutional neural network (CNN), the most expensive fully-connected and convolution layers can be implemented using
%SGEMM~\cite{chetlur2014cudnn}.
%In the presence of statistical approximation and estimation errors, the computation of deep learning is not sensitive to
%high precision~\cite{Gupta}. In this work, we focus on SGEMM optimization.

As GPUs provide higher peak floating-point performance and memory bandwidth than CPUs contemporarily, researchers tend to adopt GPUs to accelerate compute-intensive applications and operations. 
%In fact, SGEMM's performance highly relies on low-level microarchitecture features. 
Hardware vendors provide BLAS libraries specifically tuned for their own processors, such as Intel MKL~\cite{intel2007intel} and AMD ACML~\cite{amd2014} for multicore x86 CPUs and NVIDIA cuBLAS~\cite{intel2007intel} and AMD CLMath~\cite{clmath} for GPUs. 
However, we always witness performance improvement from third-party implementations over these vendor libraries. 
For example, OpenBLAS~\cite{xianyi2012openblas}, based on its hand-tuned assembly codes, achieves the highest performance in most cases on a multicore x86\jli{correct?} CPU.
Although an OpenBLAS-like library is absent from GPUs, several % ongoing 
efforts~\cite{tan,lai,nervana_sgemm_wiki,
chien, volkov} have achieved better performance than cuBLAS for (S/D)GEMM by tuning assembly codes on NVIDIA GPUs. 
However, % accomplish the microarchitecture based performance tuning on every GPU generation
the disperse work leaves two issues to be addressed for diligently tuning the program performance on the microarchitecture of every GPU generation.

\begin{itemize}
\item {\em There is a lack of a toolchain to identify GPU microarchitecture features and guide performance tuning.}
    Unlike general-purpose CPU community where a series of toolchains are available to tune performance in a bare metal
        way, only the abstract CUDA model is encouraged. NVIDIA engineers hand
        tune routines of cuBLAS and other libraries in assembly, leaving other algorithms unoptimized.
        %The main reason is the significant changes of GPU architecture
        %among generations. For example, graphic features, register bank distribution, and floating-point instructions dual
        %issue of the recent ISAs are totally different from the previous generations~\cite{fermi}. 
        Fortunately, researchers have made some initial progress on performance tuning tools, including benchmarking~\cite{mei, volkov, wong} and designing assemblers~\cite{asfermi,bernstein2012usable,decuda,maxas} on a 
specific GPU architecture, to pursue extreme performance. We attempt to develop a methodology that provides a systematic way to identify 
microarchitecture by decoding instruction formats automatically, generating ISA-compatible binaries, and benchmarking.
\item {\em There is a lack of a comprehensive understanding of SGEMM's performance in terms of low-level GPU 
microarchitecture.} Most SGEMM analyses are circumscribed at the levels of either CUDA or PTX due to short of 
bare-metal tools on GPUs. Unfortunately, these studies cannot directly diagnose compiler deficiency or hardware 
defect. In fact, by observing the disassembled code of SGEMM in CUDA, we find that the generated control code is  
inefficient in exploiting {\tt FFMA} (FP32 Fused Multiply Add) dual issue (See Figure~\ref{fig:assemblycode} in section~\ref{sec:optimization}). 
It indicates that NVCC generated code does not 
completely utilize all cores in an SM (streaming processor), leaving 
them idle at most of the time. 
A side effect of the limitation leads to a bias estimation of the performance bound. We 
present a thorough performance analysis which goes through the whole architecture hierarchy, including instruction 
throughput, register allocation, and shared/global memory transactions. The understanding of microarchitecture helps us 
build a robust performance model of SGEMM.
\end{itemize}

In this paper, we propose an GPU ISA encoding solver to crack GPU ISA encoding automatically by feeding GPU disassembly code,
and then build an 
assembler to generate CUDA binary files. Because of the compatible grammar of CUDA {\tt 
cuobjdump}~\cite{cubin2015util}, we use CUDA toolchains~\cite{nvcc} to compile CUDA codes to a {\em cubin} file and 
then disassemble it to generate assembly codes. This approach allows users to optimize any code segment on the basis
of generated codes instead of coding from scratch. By using this assembler, we design a set of microbenchmarks that 
demystify plenty of GPU microarchitecture details such as instruction issue, warp scheduling, register bank distribution, 
and control code, which are helpful to understand and optimize the performance of a computational kernel. We apply a collection of 
optimizations to improve SGEMM performance incrementally. More specifically, we make the following contributions:
\begin{itemize}
\item We propose a GPU ISA encoding solver to crack the GPU ISA encoding
    automatically for different generations of GPUs by feeding corresponding disassembly codes.
%instruction encoding of GPU architecture.
An assembler for Kepler GPU is developed to directly tune the assembly codes generated by CUDA compiler.
\item We design a set of benchmarks to reveal microarchitectural features which are undocumented by NVIDIA, 
especially for control codes that regulate {\tt FFMA} instruction dual issue.
They are necessary to understand and tune the performance of GPU programs.
\item We implement the highest performance SGEMM routines on NVIDIA Kepler by applying the demystified 
microarchitecture-level optimizations. The optimized SGEMM achieves the maximal floating-point efficiency of 88\%, 
which is 15\% higher than cuBLAS.
\end{itemize}

%Albeit Kepler is not the latest generation GPU, the {\tt FFMA} dual-issue feature will not be
%outdated. During our exploring of the method to fully exploit the arithmetic
%dual-issue feature of Kepler, we find that NVCC can not generate dual issue code efficiently even for computation
%intensive SGEMM which prevents dual issue feature further development in the succeeding GPUs. The Dual-issue feature would return in future
%GPU design when scalability of thread level parallelism and frequency of GPU are too hard to improve.
Although this work demonstrates the effectiveness on NVIDIA Kepler architecture, the approach is general-purpose for 
other NVIDIA GPU architectures by minor adjustments of instruction solver and benchmarking. 
To the best of our 
knowledge, it is the first time to provide a detailed description of how to crack the instruction encoding for NVIDIA 
GPUs~\footnote{Code:https://github.com/PAA-NCIC/PPoPP2017\_artifact} automatically. 
The experience of 
exploring bare-metal optimizations is valuable to compiler development and performance tuning.

The rest of this paper is organized as follows. Section~\ref{sec:background} introduces 
a blocking SGEMM algorithm and the CUDA binary utilities.
Section~\ref{sec:assembler} presents the instruction solver algorithms and
microbenchmarking 
insights. Section~\ref{sec:optimization} applies a series of microarchitectural optimizations to SGEMM. We report 
experimental results on section~\ref{sec:experiment}.
Section~\ref{sec:generality} discusses the generality and portability of our methodology for different GPUs and different algorithms.
Section~\ref{sec:related} summarizes the related work. Finally, 
section~\ref{sec:conclusion} concludes this work. 
