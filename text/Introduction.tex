\section{Introduction}
Single-precision General Matrix Multiply (SGEMM), as one 
of the basic routines in a BLAS library~\cite{blas,intel2007intel,amd2014}, performs a multiplication of two single-precision matrices. 
It has been extensively used in many scientific and engineering 
computing applications. 
Recently, increasing efforts have been made to tune the performance of SGEMM, because of its critical performance effect on deep learning applications~\cite{chetlur2014cudnn,nervana_sgemm_wiki}.
% as it is a performance critical kernel in
%In convolutional neural network (CNN), the most expensive fully-connected and convolution layers can be implemented using
%SGEMM~\cite{chetlur2014cudnn}.
%In the presence of statistical approximation and estimation errors, the computation of deep learning is not sensitive to
%high precision~\cite{Gupta}. In this work, we focus on SGEMM optimization.

As GPUs provide higher peak floating-point performance and memory bandwidth than CPUs contemporarily, researchers tend to adopt GPUs to accelerate compute-intensive programs. 
%In fact, SGEMM's performance highly relies on low-level microarchitecture features. 
Hardware vendors provide BLAS libraries specifically tuned for their own processors, such as Intel MKL~\cite{intel2007intel} and AMD ACML~\cite{amd2014} for multicore x86 CPUs and NVIDIA cuBLAS~\cite{intel2007intel} and AMD CLMath~\cite{clmath} for GPUs. 
However, we always witness performance improvement from third-party implementations over these vendor libraries. 
For example, OpenBLAS~\cite{xianyi2012openblas}, based on its hand-tuned assembly codes, achieves the highest performance in most cases on a multicore x86\jli{correct?} CPU.
Although an OpenBLAS-like library is absent from GPUs, several % ongoing 
efforts~\cite{tan,lai,nervana_sgemm_wiki,
chien, volkov} have achieved better performance than cuBLAS for (S/D)GEMM by tuning assembly codes on NVIDIA GPUs. 
However, % accomplish the microarchitecture based performance tuning on every GPU generation
the disperse work leaves two issues to be addressed for diligently tuning the SGEMM performance on the microarchitecture of every GPU generation.
\begin{itemize}
\item {\em A lack of a toolchain to identify GPU microarchitecture features and guide performance tuning.}
  Unlike the general-purpose CPU community where a series of toolchains are available to tune performance in a bare-metal
  way, only the abstract CUDA model is encouraged in the GPU community. 
  NVIDIA engineers hand tune their supported % cuBLAS routines and other vendor
  libraries in GPU assembly language, this leaves other unsupported algorithms hard to be diligently optimized.
  %The main reason is the significant changes of GPU architecture
  %among generations. For example, graphic features, register bank distribution, and floating-point instructions dual
  %issue of the recent ISAs are totally different from the previous generations~\cite{fermi}. 
  Fortunately, researchers have made some initial progress on performance tuning tools, including benchmarking~\cite{mei, volkov, wong} and designing assemblers~\cite{asfermi,bernstein2012usable,decuda,maxas} on some particular GPU architectures\jli{further check}.
% to pursue extreme performance. 
In this paper, we develop a methodology to systematically identify 
microarchitecture by automatically decoding instruction formats, generating ISA-compatible binaries, and benchmarking.
\item {\em A lack of comprehensive understanding of the SGEMM performance behavior in terms of the low-level GPU microarchitecture.} 
Most SGEMM analyses are circumscribed at CUDA or PTX level due to the short of GPU bare-metal tools. 
Therefore, these studies cannot directly diagnose compiler deficiency or hardware 
defects. 
By observing the disassembled codes of CUDA SGEMM, we find that the CUDA compiler (NVCC) generated control codes are deficient in exploiting FP32 Fused Multiply Add ({\tt FFMA}) dual issue (details in section~\ref{sec:ffma-dual}). 
This indicates the binary codes cannot completely utilize GPU cores but leave some of them idle most of the time. 
% completely utilize all cores in a streaming processor (SM), 
A side effect of this compiler deficiency leads to a bias estimation of the performance bound. 
This paper presents a thorough performance analysis which goes through the whole architecture hierarchy, including instruction 
throughput, register allocation, and shared/global memory transfers. 
The identified microarchitecture features help us build robust performance models of SGEMM and other programs.
\end{itemize}

In this paper, we propose a GPU ISA encoding solver to automatically crack GPU ISA encodings from GPU disassembly codes,
then we build a GPU assembler to generate binaries from hand-optimized assembly codes.\jli{further check}%CUDA binary files. 
Taking the advantage of the compatible grammar of CUDA {\tt cuobjdump}~\cite{cubin2015util} with NVCC~\cite{nvcc}, we use NVCC to compile CUDA codes to a {\em cubin} file and 
then disassemble it to assembly codes. 
This approach allows users to optimize any code segment based on the generated assembly codes instead of coding from scratch. 
With this assembler, a microbenchmark suite is designed to 
demystify plenty of GPU microarchitecture features such as instruction issue, warp scheduling, and register bank distribution, which help understand and optimize the performance of a computational program. 
We apply optimization methods corresponding to GPU microarchitecture features to improve SGEMM performance incrementally. 
More specifically, we make the following contributions:

\begin{itemize}
\item We propose a GPU ISA encoding solver to automatically crack ISA encodings
     of diverse GPU microarchitectures from disassembly codes.
%instruction encoding of GPU architecture.
A Kepler GPU assembler is developed to directly tune the assembly codes generated by CUDA compiler.
\item We design a microbenchmark suite to explore undocumented
microarchitecture features of NVIDIA GPUs, such as control codes regulating
{\tt FFMA} instruction dual issue and register bank indices influence
instruction throughput, which are necessary to understand and tune GPU
programs.
\item We implement the SGEMM routine on a NVIDIA Kepler GPU by applying the demystified microarchitecture-level optimizations. 
The optimized SGEMM achieves up to 88\% of the machine's peak floating-point performance, which is 15\% higher than the state-of-the-art SGEMM in cuBLAS.
\end{itemize}

%Albeit Kepler is not the latest generation GPU, the {\tt FFMA} dual-issue feature will not be
%outdated. During our exploring of the method to fully exploit the arithmetic
%dual-issue feature of Kepler, we find that NVCC can not generate dual issue code efficiently even for computation
%intensive SGEMM which prevents dual issue feature further development in the succeeding GPUs. The Dual-issue feature would return in future
%GPU design when scalability of thread level parallelism and frequency of GPU are too hard to improve.
Although this work demonstrates the effectiveness of our methodology on a NVIDIA Kepler GPU, the methodology is general-purpose for 
other NVIDIA GPU architectures under minor adjustments of instruction solver and benchmarking. 
% To the best of our knowledge, it is the first time to provide a detailed description of how to crack the instruction encoding for NVIDIA GPUs automatically.
To the best of our knowledge, it is the first time to thoroughly describe the automatic cracking of the instruction encodings of NVIDIA GPUs
\footnote{\scriptsize{Code:https://github.com/PAA-NCIC/PPoPP2017\_artifact}}. 
The experience of 
exploring bare-metal optimizations is helpful\jli{``valuable'' may be too much} to compiler development and performance tuning.

The rest of this paper is organized as follows. Section~\ref{sec:background} introduces 
a general blocking SGEMM algorithm and the CUDA binary utilities.
Section~\ref{sec:assembler} presents the instruction solver algorithms and
microbenchmarking 
insights. Section~\ref{sec:optimization} applies a series of microarchitectural optimizations to SGEMM. We report 
experimental results in section~\ref{sec:experiment}.
Section~\ref{sec:generality} discusses the generality and portability of our methodology for different GPUs and diverse algorithms.
Section~\ref{sec:related} summarizes the related work. Finally, 
section~\ref{sec:conclusion} concludes this work. 
