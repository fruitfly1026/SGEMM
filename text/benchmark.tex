\subsection{Correlating Microarchitecture with Performance\jled{change to ``Benchmarking''?}}
\label{sec:benchmark}


To explore NVIDIA Kepler architecture, we a create microbenchmark tailored for each characteristic.
% Our following observations are drawn from analyzing the execution time of the microbenchmarks.
Each program from the microbenchmark is basically a main function wrapping CUDA kernel code with timing around.
Timing ticks are recorded in a general register by reading the clock register (using clock()) and then time is calculated and written to global memory.
% The clock values are
% first stored in registers, then written to global memory at the
% end of the kernel to avoid slow global memory accesses from
% interfering with the timing measurements.
For timing accuracy, the same CUDA kernel code is duplicated for several times, then the average is used for our analysis.
% The general structure of a microbenchmark consists of GPU
% kernel code containing timing code around a code section (typically an unrolled loop running multiple times) that exercises
% the hardware being measured. 
All CUDA kernel codes are designed small enough to fit into the L1 instruction cache.
We run each benchmark program twice, disregarding the first run to avoid compulsory instruction cache misses. 
Other than timing information, we also use instruction latency and throughput as performance criteria.
Instruction throughput is defined as the maximum number of a particular type of launched instructions per a clock cycle.
% when the operands of each instruction are independent of the preceding instructions.
We use similar testing method with Fog's in~\cite{fog}.


We tune the assembly codes of our microbenchmark using the assembler in Figure~\ref{fig:workflow}.
From the tuning process, we correlate microarchitecture characteristics with the benchmark performance, which is useful to optimize program snippets in real applications.
% According to the performance tuning results, we 
The correlation is presented as some meaningful observations, which are categorized into four microarchitectural features:  {\tt 
control} function, {\tt register} allocation, {\tt arithmetic} throughput, and {\tt memory} operation.


\begin{figure*}
    \begin{subfigure}[htbp]{0.3\textwidth}
        \includegraphics[width=2.1in]{ctrl}
       \subcaption{Regulate {\tt FFMA} throughput. \jled{8-bit control code value} \jled{ops/cycle? or IPC?}}
        \label{fig:control_throughput}
    \end{subfigure}
    \begin{subfigure}[htbp]{0.3\textwidth}
        \includegraphics[width=2.1in]{ctrl_latency}
        \subcaption{Regulate {\tt FFMA} latency. \jled{8-bit control code value}}
        \label{fig:control_latency}
    \end{subfigure}
    \begin{subfigure}[htbp]{0.3\textwidth}
        \includegraphics[width=2.1in]{pattern}
        \subcaption{Peak {\tt FFMA} throughput(S: single issue, D: dual issue). \jled{Y-axis is performance?}}
        \label{fig:control_pattern}
    \end{subfigure}
    \caption{Different control code impact on performance}
    \label{fig:control_code}
\end{figure*}


{\em {\bf Observation 1--[Control]}: 
Warp scheduling and issue mode are tunable by modifying control codes which regulate instruction execution.}

Starting with the Kepler architecture, NVIDIA has been moving some control logics off the chip and into kernel 
instructions which are determined by the assembler~\cite{lai,maxas}. This evolution provides programmers the opportunity to 
make globally optimal scheduling decisions and other control optimizations if an assembler is available. The disassembly code from sample programs and cuBLAS library indicates that every $64$-bit control binary code controls \jled{at most} $7$ instructions as Figure~\ref{fig:assemblycode}.
% a control code is placed before $7$ instructions, and it 
We identify that both the highest $6$ bits and lowest 
$2$ bits are {\em opcodes}, and the middle $56$ bits are used to control the execution of $7$ 
instructions, each of which is assigned to $8$ control bits.

For the $8$ control bits, we identify their meanings by statically examining the disassembly codes of cuBLAS and 
dynamically benchmarking instruction sequence of different control codes.
In this way, we discover bit-$4$ marks global memory dependency barrier, bit-$5$ represents shared memory dependency barrier, and bit-$7$ indicates a texture cache dependency barrier due to its weak consistent memory model. 
Some unexpected values are loaded if the any $4-7$ bits is not set. 
\jled{Wrong bit number, bit-4 not 4th bit! What about bit-$6$?? Please check the correctness.} 
% We verify the $7$\textsuperscript{th} bit of the control 
% code of {\tt TEXDEPBAR} instruction is set to $1$, to
% indicate a texture dependency barrier of texture cache due to its weak consistent memory model. 
We use {\tt FFMA} instruction's behaviors to crack $0-3$ bits of its control code.
Figure~\ref{fig:control_code}(\subref{fig:control_throughput}) and 
Figure~\ref{fig:control_code}(\subref{fig:control_latency}) show {\tt FFMA}'s throughput and latency  when its $8$ control bits varies from $0$ to $255$. 
% Thus, most throughputs achieve the maximum when the control code's value is smaller than {\tt 0x20} because of no set suspension.
% Obvious periodicity is shown with size $16$ after the control code's value reaches {\tt 0x20}.
After {\tt 0x20}, both the throughput and latency show obvious periodicity.
For each period, by increasing the value of $0-3$ bits, the {\tt FFMA} throughput drops and its latency raise by different rates. \jled{the period size is different! why?}
This phenomenon implies that the $4$ lower bits stores the number of stall cycles before issuing 
an instruction. 
Our microbenchmarking reveals some specific patterns of control codes:
% Furthermore, the microbenchmarking reveals some specific patterns of control codes:

\begin{itemize}
\item When the control bits are set to be {\tt 0x00}, the scheduler suspends a warp of the instructions for $16$ cycles.
\item {\tt 0x2n} means a warp is suspended for $n$ cycles before issuing the next instruction, where $n=0, \dots, 15$.
\item {\tt 0x00} means single issue mode, while {\tt 0x04} means dual-issue mode. 
While two consecutive instructions are controlled by {\tt 0x04} and {\tt 0x05} respectively, the throughput could reach the maximum. \jled{relation with Figure?}
\end{itemize}


{\em {\bf Observation 2--[Register]}: 
Irrespective of single- or dual-issue mode, only source operands may cause register bank conflicts that degrades instruction throughput.}

Shared memory bank conflict is well-known as an important performance factor for CUDA programming.
Recent research~\cite{lai} noticed that register bank conflict is also nontrivial to performance. 
In order to probe 
register bank conflict, our microbenchmark measures instruction throughput for different combination of {\tt FFMA} 
register operands. 
Table~\ref{tab:th} shows one example combination resulting in various efficiency numbers. 
The rightmost column represents the number of register bank conflicts recorded from our experiments. 
This experiment is conducted 
in single issue mode by setting control code to 0x20 \jled{0x00 is single-issue mode ?}. 
Theoretical, the peak efficiency is $128/192=66.67\%$ \jled{why 128?}. 
In fact, we observe that both single- and dual-issue mode produce the same throughput behavior for bank conflicts.
 % variance of instruction throughput. 
From our experiments on Kepler architecture, we observe:
\begin{itemize}
\item Destination operand will not contribute to bank conflict, no matter which bank is assigned to it.
\item When source operands have 2- or 3-way register bank conflict, the throughput will drop up to 2.33\% and 17.17\% respectively in single issue mode. 
% When source operands have 3-way conflicts, the throughput will drop by up to 17.17\%. 
    \jled{Is the two numbers fixed?}
\item Our microbenchmark finds out a proper distribution of registers to eliminate bank
     conflict. 
     The distribution is summarized in Table~\ref{tab:reg}, which confirms the rule in~\cite{lai}: \\
 bank0$\Leftarrow$($Ridx \% 8 < 4$ \&\& $Ridx \% 2 == 0$) \\
 bank2$\Leftarrow$($Ridx \% 8 < 4$ \&\& $Ridx \% 2 == 1$) \\
bank1$\Leftarrow$($Ridx \% 8 > 4$ \&\& $Ridx \%2 == 0$) \\
bank3$\Leftarrow$($Ridx \% 8 < 4$ \&\& $Ridx\% 2 == 1$)\\
where $Ridx$ is the register number. 
This rule will guide the performance tuning of SGEMM code.

\end{itemize}

\begin{table}[htbp]
    \caption{The efficiency of instruction throughput varies with difference register bank distribution. {\it Inst} : 
instruction pattern, {\it Th/SM}: the instruction throughput per SM, {\it Eff}: efficiency of throughput, {\tt Conf}: register bank conflicts.}
\centering
\scalebox{1.0} {
\begin{tabular}{|c|c|c|c|}
\hline
Inst &Th/SM&Eff&Conf\\
\hline
{\tt FFMA R5,R4,R1,R0}&127.50&66.40\%&0\\
\hline
{\tt FFMA R2,R4,R1,R0}&127.50&66.40\%&0\\
\hline
{\tt FFMA R5,R2,R1,R0}&119.18&62.07\%&2-way\\
\hline
{\tt FFMA R3,R2,R1,R0}&119.18&62.07\%&2-way\\
\hline
{\tt FFMA R5,R9,R3,R1}&94.52&49.23\%&3-way\\
\hline
{\tt FFMA R11,R9,R3,R1}&94.52&49.23\%&3-way\\
\hline
{\tt FMUL R4,R1,R0}&127.50&66.40\%&0\\
\hline
{\tt FMUL R4,R2,R0}&119.17&62.06\%&2-way\\
\hline
\end{tabular}
}
\label{tab:th}
\end{table}


\begin{table}[htbp]
\caption{Register bank distribution.}
\centering
\scalebox{1.0} {
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
    {\tt Bank0}&{\tt R0}&{\tt R2}&{\tt R8}&{\tt R10}&{\tt R16}&{\tt R18}&{\tt R24}&{\tt R26}\\
\hline
    {\tt Bank1}&{\tt R1}&{\tt R3}&{\tt R9}&{\tt R11}&{\tt R17}&{\tt R19}&{\tt R25}&{\tt R27} \\
\hline
    {\tt Bank2}&{\tt R4}&{\tt R6}&{\tt R12}&{\tt R14}&{\tt R20}&{\tt R22}&{\tt R28}&{\tt R30}\\
\hline
    {\tt Bank3}&{\tt R5}&{\tt R7}&{\tt R13}&{\tt R15}&{\tt R21}&{\tt R23}&{\tt R29}&{\tt R31}\\
\hline
\end{tabular}
}
\label{tab:reg}
\end{table}

{\em {\bf Observation 3--[Arithmetic]}: 
With a proper control code pattern and register allocation, {\tt FFMA} 
instruction throughput can approach the theoretical peak in dual issue mode.}

It's very intricate to tune instruction execution to improve instruction throughput. The previous work~\cite{lai} 
reports the maximal throughput of {\tt FFMA} per SM as $132$, which is much lower than the theoretical throughput ($192$) on Kepler. 
Our microbenchmark reveals several key points of optimization to approach theoretical peak on Kepler. 
First, the control code must be set properly to dual issue adjacent instructions. Second, the ratio and interval of 
dual issue {\tt FFMA} instructions must be tuned into a specific pattern. 
Since each warp of extra computing unit is 
shared among two warps, \jled{non clear} when all threads are trying to fully dual issue every two adjacent {\tt FFMA}s, half of the 
scheduler would stall due to computing resource conflict. 
Thus, the optimal ratio of dual issue to single issue is $2:2$ theoretically, and 
with a proper phase shift among two warp's executing pace, they could get access to the shared computing unit in turn. \jled{not clear}
Using the $2:2$ ratio, \( \begin{pmatrix} 4 \\ 2 \end{pmatrix} \) $=6$ combinations for mixed single  and dual issue pattern inside a $7$ instructions scheduling block (Figure~\ref{fig:control_code}(\subref{fig:control_pattern})). 
We choose the best {\tt SDDS} pattern in our SGEMM implementation. 
Third, the first instruction of the core loop needs to be aligned. This restriction is 
caused by the aligned position of control code in the instruction sequence. 
Last, {\tt FFMA} dual issue requires $6$ register banks from Table~\ref{tab:reg} \jled{Where is 6 from? Table~\ref{tab:reg}, right?}. 
Instruction order has to be adjusted to fully use Kepler's operand 
collector mechanism~\cite{collector,tarjan2012policy} to avoid register bank conflicts.
\jled{A new concept: operand collector mechanism. Maybe just don't mention it.}
As shown in 
Table~\ref{tab:ffma}, these optimizations together improve {\tt FFMA}'s throughput to $190$ ops/clock \jled{ops/cycle?}, which is very close to the theoretical peak $192$ ops/clock.

\begin{table}[htbp]
\caption{Floating-point instruction throughput on Kepler}
\centering
\scalebox{1.} {
\begin{tabular}{|c||c|c|c|}
\hline
Inst &operation&single issue&dual issue\\
\hline
{\tt FFMA} &c=a*b+c&127.52&190.35 \\
\hline
{\tt FMUL} &c=a*b&127.52&190.35 \\
\hline
{\tt FADD} &c=a+b&127.52&191.50\\
\hline
\end{tabular}
}
\label{tab:ffma}
\end{table}


{\em {\bf Observation 4--[Memory]}: For higher memory bandwidth, shared memory prefers 64-bit load 
instruction {\tt LDS.64} while global memory prefers the load instruction with texture cache {\tt LDG}.}

For GPU memory hierarchy we focus on the programmer controllable memory resources, shared memory and global memory. 
On NVIDIA GPU architecture, there are different memory access widths (32-bit, 64-bit, 128-bit) and paths (through normal or texture cache). 
In fact, both NVIDIA documents and previous studies~\cite{tan} pointed out that wider 
instructions have longer pipeline latency.
Our benchmarking confirms this phenomenon and also identifies several bandwidth issues for memory optimization.


\jled{operations refer to instructions here?}
Intuitively, a wider load operation should achieve higher bandwidth. 
We test the bandwidth of shared memory operations with different widths, {\tt LDS.32}, {\tt LDS.64},
and {\tt LDS.128}. 
The operations are specially arranged to avoid shared memory bank conflict. 
% In this experiment, the amount of data are projected to the number of load instructions. 
Assume the data can be loaded by $N$ {\tt LDS.128} instructions, 
then $2N$ {\tt LDS.64} or $4N$ {\tt LDS.32} instructions are needed.
Figure~\ref{fig:lds_bw} compares the sequential memory access bandwidth of the three instructions by increasing the data volume. 
{\tt LDS.64} achieves the highest bandwidth $137GB/s$, which is about $76\%$ of the peak bandwidth\footnote{The 
theoretical shared memory bandwidth for each SM can be calculated as $Bandwidth = f_{core} \times Width \times Warpsize$ in
bytes, where $f_{core}$ is the frequency of a CUDA core, $Width$ is bank width, $Warpsize$ is warp size.}.

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{lds_bandwidth}
    \caption{ Bandwidth of {\tt LDS.32}, {\tt LDS.64} and {\tt LDS.128}}
\label{fig:lds_bw}
\end{center}
\end{figure}

% global memory
Two paths are used to load data from global memory, through L2 cache by {\tt
LD} instruction or through texture cache by {\tt LDG} instruction. 
We launch $26$ thread blocks each with $512$ threads, and specifies that each thread accesses $4$ words in a stride of $4 \times blockDim.x \times gridDim.x$. 
Our benchmark confirms that {\tt LDG} achieves higher bandwidth than {\tt LD}.
% , which has been identified by previous work~\cite{tan}.
