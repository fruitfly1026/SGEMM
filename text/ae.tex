\section{Artifact description}

%Submission and reviewing guidelines and methodology: \\
%{\em http://cTuning.org/ae/submission-20160509.html}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Abstract}

The artifact contains three parts of our work: KeplerAs assembler, optimized SGEMM code, and reverse-engineering GPU ISA encoding code. It can support the Algorithm $2$, $3$ and Figure $9$, $10$ in our PPoPP 2017 paper: Demystifying GPU Microarchitecture to Tune SGEMM Performance. To validate the results, run the test scripts and check the results in the according text output files.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Description}

\subsubsection{Check-list (artifact meta information)}
%{\em Fill in whatever is applicable with some informal keywords and remove the rest}

{\small
\begin{itemize}
  \item {\bf Algorithm: SGEMM algorithm, KeplerAs assembler, GPU ISA encoding solver}
  \item {\bf Program: CUDA, C/C++ code, python and GPU assembly code}
  \item {\bf Compilation: nvcc, g++, python,perl and KeplerAs.pl (after install our GPU assembler KeplerAs)}
  %\item {\bf Compile: }
  \item {\bf Binary: Cubin and CUDA executables}
  \item {\bf Data set: Input matrix is filled random generated data; solver's input is generated by script}
  \item {\bf Run-time environment: Redhat 6, CUDA driver 7.0}
  \item {\bf Hardware: NVIDIA K20 GPU, Intel Xeon CPU}
  %\item {\bf Run-time state: }
 % \item {\bf Execution: }
  \item {\bf Output: Gflop/s of SGEMM and encodings of Kepler ISA}
  \item {\bf Experiment workflow: git clone project; run the test scripts; observe the results in text files}
  %\item {\bf Experiment customization: }
  \item {\bf Publicly available?: Yes }
\end{itemize}
}

\subsubsection{How delivered}
Our code is an open source under Apache 2.0 license, and is hosted on github.

\subsubsection{Hardware dependencies}
Intel Xeon CPU and NVIDIA K20 GPU
\subsubsection{Software dependencies}
Red Hat Enterprise Linux Server release 6.3 (Santiago)

CUDA driver 7.0

KeplerAs GPU assembler (Inside our code)

Python 2.7

Perl v5.10.1

Awk 3.1.7

\subsubsection{Datasets}
Dataset is generated during running, so no dataset is required.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Installation}
Install software listed in software dependencies section.

%{\em Obligatory}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment workflow}
For the convenience of the artifact evaluation, we provide a series
of shell scripts which run the solver described in Algorithm 2 and 3 of the paper and store the results in the output text files. Below are the steps to download our code, build, run the experiments, and observe the results.

Clone https://github.com/PAA-NCIC/PPoPP2017\_artifact, then follow commands below.

Install the assembler.
\begin{lstlisting} [frame=tb, basicstyle=\ttfamily\footnotesize]
$ cd PPoPP2017_artifact/KepelerAs
$ perl Makefile.PL
$ make
$ sudo make install
\end{lstlisting}

Run SGEMM algorithm to evaluate performance, the script "./sgemm.sh" will compile assembly to Cubin and run the experiments. The output is ppopp\_time.txt which contains Gflop/s of our SGEMM and cuBLAS SGEMM.
The executable "sgemm" will compare the result of our SGEMM with cuBLAS's SGEMM to make sure that our implementation is correct.

$12\times12$ blocking SGEMM performance vs. cuBLAS performance.
\begin{lstlisting} [frame=tb, basicstyle=\ttfamily\footnotesize] 
$ cd SGEMM/12x12
$ ./sgemm.sh
\end{lstlisting}

$8\times8$ blocking SGEMM performance vs. cuBLAS performance.
\begin{lstlisting} [frame=tb, basicstyle=\ttfamily\footnotesize] 
$ cd SGEMM/8x8
$ ./sgemm.sh
\end{lstlisting}

Solver is composed of opcode solver, modifier solver and operand solver.
The output ISA encoding information will be used to build a GPU assembler.
\begin{lstlisting} [frame=tb, basicstyle=\ttfamily\footnotesize] 
$ cd Solver/
$ ./solver.sh
\end{lstlisting}
%{\em Obligatory}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation and expected result}
%{\em Obligatory}
First, our KeplerAs GPU assembler is expected to compile assembly code into Cubin correctly. This can be validated by compiling .sass file in SGEMM directory.
Second, Figure $9$ and Figure $10$ in our paper presented the performance of our optimized SGEMM and cuBLAS SGEMM. We expect results contain Gflop/s by running sgemm experiment. 
Third, we expect the solver to crack ISA encodings automatically, and to generate opcode, operand and modifier encodings information in corresponding text files(operand.txt, modifier.txt). These information can be used to build a GPU assembler.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Experiment customization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notes}
The KeplerAs compiler installation time and SGEMM evaluation time are short.
The solver might run a little bit long. Running opcode solver, modifier solver and
operand solver would take 10 minutes, 30 minutes and 20 minutes respectively on
Intel(R) Xeon(R) CPU E5-2670. If you have any problem in validating the above, please contact me.
