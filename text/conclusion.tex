\section{Conclusion}
\label{sec:conclusion}
We presented a methodology to demystify GPU microarchitectural features and demonstrated its 
application on tuning SGEMM and convolutionn algorithms. 
The methdology includes a GPU ISA solver to crack its ISA encoding, a GPU
assembler and microbenchmarks at assembly level to correlate architectural
features with performance.
%The methodology relies on a reverse engineering
%approach to crack GPU ISA encoding, and a profound microbenchmarking at
%assembly level to correlate architecture features with performance factors. 
%These benchmarks figured out dual issue impact on float
%arithmetic throughput, memory load width on bandwidth, register 
%bank distribution on performance etc. 
%These features are worthwhile for both hardware and software researchers to 
%understand GPU architecture and program optimization on GPUs. 
%As far as we know, it is the first time to provide a GPU ISA encoding solver to crack the GPU ISA encoding automatically. 
%This solver is not limited to Kepler GPU, it can be easily applied to other GPU architectures. 
Based on the disclosed insights, we implemented the
fastest SGEMM,  $3104$Gflop/s with $88\%$ efficiency on a NVIDIA Tesla K20m, which is $15\%$ higher than cuBLAS. 
We also build roofline models to analyze its performance upper bound. 
Applying these optimizations on convolutional networks, the optimized implementations outperform cuDNN4.0 by 39\%-62\% on Tesla K20m.
Our work broadens GPU optimizations to a deeper native machine level,
% to optimize NVIDIA GPU code at native machine level, 
which may enlighten developments on
compilers or other performance critical kernels.
In the future, we will apply our methodology to the latest Pascal GPU to reveal its microarchitectural features.
We also intend to apply our GPU microarchitectural insights and assembly
optimization to optimize the code generation of the open source GPU compiler
GPUCC~\cite{wu2016gpucc}.
%We also considering 
%Our future work is to extend this methodology to FFT and integrate optimized SGEMM into deep learning algorithms, such as convolutional neural networks (CNN).
%\jled{I added the future work. Please check or remove it.}
% cuBLAS's SGEMM achieves $2509$ Gflop/s ($71\%$ efficiency). 
% Our optimized one is $17\%$ higher than Cublas. 
% The performance boost is achieved on the basis of tuning {\tt FFMA} throughput as high as 
% hardware peak, then add other non-FFMA instruction with little penalty. 
% These optimizations include register bank 
% conflict eliminating, {\tt FFMA} dual issue scheduling, choosing proper width of global load and shared memory load instructions and so on.
%\jled{I removed one and a half unnecessary paragraphs.}
% Although Kepler is not the latest GPU, the arithmetic dual-issue feature will not be outdated.
% During our exploring of the way to fully exploit the arithmetic dual-issue feature of Kepler, we discovered that NVCC
% can not generate dual issue code efficiently which prohibits future GPU to adopt dual issue design. 
% However when scalability of thread level parallelism
% and frequency of GPU are too hard to improve, dual-issue feature would return in future GPU design.
% The development of GPU compiler could learn experiences from these microarchitectural-level optimizations.
