\section{Evaluation}
\label{sec:experiment}

In this section, we compare the optimized SGEMM performance in Gflop/s with NVIDIA cuBLAS. 
Our SGEMM achieves 15\% %$17\%\sim 25\%$ 
performance
improvement, which confirms the usefulness of the microarchitectural optimizations on GPUs. 
We present 
a quantitative analysis of the effect of each optimization strategy and an estimation of the performance upper bound using a roofline model.

The experiments are conducted on NVIDIA Tesla K20m GPU (refer to \cite{gk110} for detailed specifications). 
%\jli{Though delete the hardware configuration figure, still need to state main parameters in words!}
We compare with cuBLAS from CUDA $7.0$ on square matrices and rectangle matrices in four shapes for $A$, $B$, and $C$. 
The size of square matrices varies from $384$ to $12288$ with a step $384$. The smallest dimension of rectangle matrices varies from $192$ to $3072$ with a step $192$.

\subsection{Overall Performance}
Figure~\ref{fig:rect} shows SGEMM performance of five different matrix shapes, [W, W, W], [W, 2W, 4W], [W, 4W, W], [4W, W, 4W], and [4W, 2W, W], where W values are shown on the x-axis.
The results demonstrate that our $12\times12$ and $8\times8$ blocking implementations are better than cuBLAS for almost all matrix shapes, while $12\times12$ is the best one when matrix greater than $1000$.
The performance of both cuBLAS and our SGEMM for matrix shape [W, 4W, W] fluctuates more seriously than other shapes, because its parallelism is coarser and each thread iteration is much longer (K=4W).
In contrast, [4W, W, 4W] with the finest parallelism of the four shapes has the smoothest results.
%Figure~\ref{fig:rect} reports the performance of cuBLAS SGEMM and our optimized SGEMM.
When matrix size is $12288\times12288$, the optimized SGEMM achieves $3104$
Gflop/s with $88\%$ efficiency, while cuBLAS gets $2705$ GFlop/s with $76\%$ efficiency.
Our SGEMM achieves $1.15 \times$ performance speedup over cuBLAS.% $12\%$ in efficiency.
SGEMM performance increases with matrix sizes. 
%\jled{the next sentence change to ``On the one hand, a larger matrix has a higher arithmetic intensity 9$AI$), which is the ratio of compulsory floating-point operations to the total DRAM memory traffic.
%The high arithmetic intensity makes good use of GPU computing resources to obtain good performance.''}
One reason is that a larger matrix has a higher ratio of 
floating-point operations to the store operations of matrix $C$, which is more close to the hardware arithmetic intensity. 
The other reason is that a larger matrix has a better load balance on GPU by increasing the workload of each CUDA core.
%\jled{change to ``number of threading blocks''}. 
The number of threading blocks ranges in $2 \times 2, 8 \times 8, \dots, 64 \times 64$ from left to right in Figure~\ref{fig:rect}.
% $[768/192,768/192]=[4,4]$ to $[12288/192, 12288/192]$ $=[64,64]$. 
Since Kepler has $13$ SMs, matrix $384\times 384$ suffers more from load imbalance because of only $4$ thread blocks.
This explains the significant performance improvement when size grows from $384$ to $1536$. 
With respect to performance improvement over cuBLAS, our optimizations benefit more on larger matrices. 
The higher arithmetic intensity of larger matrices makes their performance increasingly bounded by the GPU microarchitecture rather than its memory hierarchy. 
Therefore, our microarchitecture-level optimization plays an important role on tuning SGEMM
performance.

%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[scale=0.5]{tn_192}
%\caption{Performance comparison of cuBLAS and the optimized SGEMM on square matrices}
%\label{fig:sgemm_tn}
%\end{center}
%\end{figure}


\begin{figure*}[htbp]
\begin{center}
\includegraphics[scale=0.65]{rectangular}
\caption{\small Performance comparison of cuBLAS and the optimized SGEMM on rectangular matrices}
\label{fig:rect}
\end{center}
\end{figure*}


\subsection{Performance Analysis}

\subsubsection{Influence of Register Blocking Sizes}
Table~\ref{tab:dm} summarizes data movement volume of the blocked SGEMM algorithm.
The volume of data moving from shared memory to register is $r_x\times b_k+ r_y\times b_k$, 
and the floating-point computation volume is $r_x\times r_y\times b_k$. 
The shared memory arithmetic intensity ($sAI$) is defined as the ratio between them:
{\small
\begin{equation}
sAI = \frac {r_x\times 
r_y\times b_k} {r_x\times b_k+ r_y\times b_k} = \frac{1}{\frac{1}{r_x} + \frac{1}{r_y}}.
    \label{equ:reg_block}
\end{equation}
}
Register blocking sizes $r_x$ and $r_y$ are limited by the register counts per thread. 
Each thread needs $r_x\times r_y$, $2r_x$ and $2r_y$ registers to store the result of a matrix block of $C$,  two columns of $A$ (double-buffered software pipelining),  two rows of $B$ (double-buffering) respectively in a single iteration of for loop in Algorithm~\ref{gemm}.
% To hide shared memory latency, double-buffered software pipelining is used. 
% Thus, extra $r_x$ and $r_y$ registers
% are used to prefetch next block-column of $A$ and $B$ from shared memory. 
Since the total number of registers must be less than the maximum register counts per thread ($256$ on Kepler), we have
{\small
\begin{equation}
    r_x\times r_y + 2r_x + 2r_y < 256.
\label{equ:reg_restrict}
\end{equation}
}
Equation~\ref{equ:reg_block} implies that larger register blocking yields the higher shared memory arithmetic intensity,
and the optimal solution lies at $r_x=r_y$ with restriction~\ref{equ:reg_restrict}. 
Because data load must be aligned in $128$-bit using 
{\tt LDG.128}, the block sizes could be $4\times 4$, $8\times 8$ or $12\times 12$. 
Since $4\times 4$ leads to low data reuse in register files, we only show $8\times8$ and $12\times12$ cases in Figure~\ref{fig:rect} and \ref{fig:rect}.
$12\times12$ blocking has better performance than $8\times8$ blocking because $12\times12$ blocking has higher $sAI$ and instruction level parallelism. 
With respect to instruction scheduling optimization in Table~\ref{tab:position}, it has more slots to insert {\tt non-FFMA} instructions, and latency can be better hidden.
$12*12+4*12=192$ registers are used by {\tt LD} and {\tt FFMA} instructions, and totally $236$ registers with other address indices. 
The register counts per SM restricts us to launch up to $256$ threads/block which is in our implementation. 
Each thread block computes a matrix block $C_{192, 192}$ by multiplying $A_{192,4}$ and $B_{4, 192}$, where $4$ is the unrolling factor.

%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[scale=0.5]{block}
%    \caption{Evaluation of different blocking sizes.} %\jled{could add $4\times4$ case.}}
%\label{fig:block}
%\end{center}
%\end{figure}

Only one thread block per SM is active due to register limitation, thus the thread occupancy is $256/2048=12.5\%$.
With our high instruction level parallelism, the thread parallelism becomes low.
However, our SGEMM's high performance confirms that instruction level parallelism still plays an important role on GPU.
Similar conclusion has been mentioned by Volkov in~\cite{volkov2010better}.

\subsubsection{Profiling Microarchitectural Optimization}

To examine performance gains of different optimization strategies, we construct several intermediate 
implementations by incrementally applying our microarchitectural optimizations.
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.4]{tn_prof}
    \caption{\small Evaluation of the incremental optimizations.}
\label{fig:th_prof}
\end{center}
\end{figure}

{\it Baseline:}~~The baseline has adopted conventional optimizations including register blocking, global and shared memory double buffering, and unrolling, but without assembly-level optimizations.
For example, the baseline uses default $32$-bit {\tt LD} rather than $128$-bit {\tt LDG} instruction to load data from global memory.
Registers are allocated orderly first for $C$ from $0$ to $143$, then for $A$ and $B$. 
In this case, {\tt FFMA}s have $368/(144*4)=63.89\%$ 2-way bank conflicts and $64/(144*4)=11.11\%$ 3-way bank conflicts. 
Besides, the baseline cannot apply the dual-issue optimization either.

{\it +Reg:}~~The register allocation pattern described in Section~\ref{sec:register} is applied to eliminate register bank conflicts. 
%No optimization of instruction scheduling for this version.

{\it +LD128:}~~Use wide global load instruction {\tt LD.128} with L2-cached.
Though Kepler has a L1 data cache, it is designed for local rather than global memory access~\cite{gk110}.
% So {\tt LDx} will not be L1-cached, it can be L2-cached.

{\it +LDG128:}~~Use the faster texture cached {\tt LDG} instead of L2-cached {\tt LD} along with {\tt TEXDEPBAR} before data access due to the weak consistency of texture cache~\cite{lukyanov2014efficient}.

{\it +Dual:}~~Use the dual-issue control fully enabled by utilizing the pattern described in Section~\ref{sec:assembler}.
%Single issue is controlled by setting control code to $0x40$. %\jled{different from section~\ref{sec:benchmark}. The left sentence is useless.}. 
In the dual-issue mode, {\tt NOP} may be inserted for the 7-instruction alignment.

Figure~\ref{fig:th_prof} illustrates performance gains of each optimization strategy.
As long as the number of instructions changes, instruction rescheduling is needed to achieve good performance.
Compared with the baseline implementation, SGEMM gains up to $2.6\times$ speedup by applying all the optimizations.
Eliminating register bank conflicts improves around $10\%$ performance. 
Wide load instruction {\tt LD.128} contributes $27\%$-$35\%$ to the performance, texture-cached load instruction {\tt LDG.128} improves $5\%$-$12\%$.
Dual issue achieves the highest improvement $84\%$-$106\%$.
%improves the most, $84\%-106\%$.


\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.4]{roofline-all}
    \caption{\small Memory roofline model using log-log scale. ``GM/SM Bandwidth'' means global/shared memory's theoretical
    bandwidth.} %Horizontal lines shows different Gflops.}
\label{fig:roofline_all}
\end{center}
\end{figure}

%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[scale=0.32]{roofline_sm}
 %   \caption{Shared memory roofline model using log-log scale. ``SM Bandwidth'' means shared memory's theoretical
%    bandwidth. }% Horizontal lines shows different Gflops.}
%\label{fig:roofline_shared}
%\end{center}
%\end{figure}


\subsubsection{Upper Bound Analysis}

We estimate the upper bound factors: 
%{\tt LDS}, {\tt LDG} and {\tt FFMA} which correspond to three kinds of resources respectively, 
shared memory, global memory, and computation power. In the register blocking loop\jli{inner loop of Algorithm~\ref{gemm}}, each thread block computes $b_m \times b_n \times b_k$ operations and reads $b_m \times b_k+b_n \times b_k$ words. The upper bound of global memory bandwidth can be modeled as:
{\small
\begin{equation}
    \frac{2 b_m \times b_n \times b_k}{4 (b_m \times b_k + b_n \times b_k)} = \frac{PeakFlops}{Bandwidth_{global}}
    \label{equ:global}
\end{equation}
}
According to parameters in our SGEMM implementation, $b_m=b_n=192$, $b_k=4$, $PeakFlops=3520$Gflop/s, so $73$GB/s is the minimum
requirement for global memory bandwidth in order to achieve the theoretical peak $3520$Gflop/s.
For shared memory, inside each loop $(r_x \times b_k + r_y \times b_k) \times t_x \times t_y$ words are read from shared memory, where $t_x$ and
$t_y$ are thread block sizes, $r_x$ and $r_y$ are register blocking sizes. The computation flops is $b_m \times b_n \times b_k$, then the ratio of computation to shared memory access is
{\small
\begin{equation}
    \frac{2 b_m \times b_k \times b_n}{4 t_x \times t_y \times (r_x \times b_k + r_y  \times b_k)}  = \frac{PeakFlops}{Bandwidth_{shared}}
    \label{equ:shared}
\end{equation}
}

The minimum shared memory bandwidth requirement of SGEMM is
$1173$GB/s on Tesla K20m, which is $1173/13=90$GB/s for each SM. 
Theoretically, hardware provides
$200$GB/s global memory bandwidth and $2349$GB/s shared memory bandwidth, which are higher than requirements, hence neither of them would be the bottleneck.

SGEMM has a different computation to memory access ratio in terms of global memory and shared memory, so we demonstrate
two roofline models in Figure~\ref{fig:roofline_all}. % and~\ref{fig:roofline_shared}. 
The slope of slant line in Figure~\ref{fig:roofline_all}(a) is global memory bandwidth. 
The x-axis represents the flops:bytes ratios which are $48$ for $192\times192$ and $32$ for $128\times128$ shared memory blocking of SGEMM by Equation~\ref{equ:global}.
% the computation to global memory access ratio of an algorithm. 
% By Equation~\ref{equ:global}, the flops:bytes ratio of $192\times192$ and $128\times128$ shared memory blocking of SGEMM are $48$ and $32$ respectively. 
Horizontal lines
show the machine's peak performance, the performance of our optimized SGEMM with or without dual issuing.
This figure demonstrates that SGEMM is not bounded by global memory bandwidth
for either $128\times128$ or $192\times 192$ shared memory blockings.
The slope of slant line in Figure~\ref{fig:roofline_all}(b) is the bandwidth of shared memory. 
The x-axis represents the flops:bytes ratios which are $3$ for $12\times12$ and $2$ for $8\times8$ register blocking of SGEMM by Equation~\ref{equ:shared}.
This figure shows SGEMM is not bounded by shared memory through our {\tt LDS} optimization. 
However, if we use {\tt LDS.128} instead of {\tt LDS.64}, SGEMM will be bounded by shared memory even for $12\times 12$ blocking.

The loss of peak performance can be explained by the following reasons. As we have shown in 
Section~\ref{sec:assembler}, {\tt FFMA} throughput can achieve $97.67\%$. 
The loss is about $2.33\%$, which may come 
from the overhead of warp scheduler in the {\tt FFMA} dual-issue mode. The double-buffering algorithm can amortize the latency 
of {\tt LDS}.
With $12\times12$ register blocking and $4$ times unrolling, there will be $144*4=576$ {\tt FFMA}s in the loop.
With our designed {\tt FFMA} dual issue pattern, every six {\tt FFMA}s needs four clock cycles in the pipeline.
It needs $4\times144\times4/6=384$ cycles for each thread, two {\tt LDG.128}s are needed.
We observe each {\tt LDG} has $10$ cycles penalty, the total {\tt LDG} will cause $2\times10/384 = 5.2\%$ loss. Other
penalties come from synchronization and writing $C$ matrix in the block.
