\begin{abstract}
	In this paper, we present thorough experience on tuning single precision general matrix multiplication (SGEMM) 
in assembly level on NVIDIA GPUs. First, we propose a methodology to demystify GPU's microarchitecture level 
optimizations. An instruction solver is developed to retrieve the undocumented ISA specification, which enables us to 
build an assembler that generates CUDA binary compatible files. We further design a rigorous microbenchmark to identify 
four microarchitectural features of control function, register allocation, arithmetic throughput, and memory operation. 
Second, we tune SGEMM by applying the correlated microarchitectural optimizations including maximizing FFMA throughput 
(core), eliminating register bank conflict (register), and selecting data path to shared/global memory (memory). The 
optimized SGEMM achieves the maximal floating-point efficiency of 88\%, which is 17\% higher than CUBLAS's 71\% on a 
NVIDIA Kepler GPU.
\end{abstract}

