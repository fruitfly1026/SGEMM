\section{Related Work}
\label{sec:related}
To our knowledge, this paper provides the first comprehensive study of demystifying NVIDIA GPU microarchitecture which i
s correlated with performance tuning in SGEMM. This section briefly discusses related work in assembler, microbenchmarki
ng and SGEMM optimization.

The lack of assembler for public use motivates a series of work on developing toolchains to facilitate tuning codes in a
ssembly level. For the early architecture G80, Decuda~\cite{decuda} demonstrated the feasibility to operate the limited 
number of assembly instructions. Then, for almost each new generation of CUDA architecture, there are several efforts on
 developing assembly toolchains. Both Hou's Asfermi~\cite{asfermi} and Bernstein's cudaasm-qhasm~\cite{bernstein2012usab
le} are assemblers for Fermi architecture. Gray built MaxAs~\cite{maxas} assembler for Maxwell architecture by reverse e
ngineering the encoding of Maxwell GPU. Our work provide a complete assembler for Kepler architecture. A remarkable adva
ntage of our assembly toolchain is the compatibility with CUDA's {\tt cuobjdump}. As a comparison, although Envytools~\c
ite{envytools} supports PTX instruction translated to binary $64$ bit instructions for serval GPU architectures, it cann
ot generate a compatible {\tt cubin} format which directly used by the CUDA driver APIs. Besides, no prior work presents
 the detailed instruction decoding algorithms process as ours.

We share the same idea of performance microbenchmarking with other works. Wong et.al.~\cite{wong} performed a comprehens
ive benchmarking work on GT200 and provided pipeline latency data and
memory feature. Mei~\cite{mei} benchmarked memory hierachy
of Fermi, Kepler and Maxwell GPU, which including cache, shared memory etc. However neither of them benchmarked vectoriz
ed load instruction like {\tt LD} and {\tt LDS}. Due to lack of considering vectorized load instructions and too less in
struction inside loop, the results is not so accurate. Neither of them considered dual issue modes of arithmetic instruc
tions. We leverage the our complete assembler to crack control codes which reveal more microarchitecture details for tun
ing application performance.

With respect to GEMM optimization in microarchitectural level, there are several work which inspires the implementation 
on specific GPUs. For example, we adopt the proved effective optimization techniques like shared memory/register blockin
g and double-buffering~\cite{volkov}~\cite{tan}. Further, Tan et.al.~\cite{tan} implemented fast DGEMM by using assembly
 level optimization, such as software pipelining, vector memory operations, instruction scheduling. Lai~\cite{lai} prese
nted performance analysis and optimization work of SGEMM on both Fermi and GTX680 GPUs. However, Lai didn't consider dua
l issue mode by setting control code, which can boost performance significantly. Scott Gray~\cite{nervana_sgemm_wiki} pr
esented optimization of SGEMM in assembly on Maxwell GPU. Since Maxwell does not support {\tt FFMA} dual issue, the opti
mization is much easier than
Kepler. In contrast, we present a more complete case of applying microarchitectural features by combining instruction sc
heduling, register allocation and memory access paths. 
