% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
\usepackage{amsmath}
\usepackage{url}
\usepackage{subfigure}
\usepackage{listings}
\usepackage{eqnarray}

\usepackage{graphicx}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{diagbox}

\graphicspath{{./Figures/}}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}
\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\doi{10.475/123_4}

% ISBN
\isbn{123-4567-24-567/08/06}

%Conference
\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

\acmPrice{\$15.00}

%
% --- Author Metadata here ---
\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Demystifying Microarchitecture in GPU to Tune SGEMM Performance}
\subtitle{}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\maketitle
\begin{abstract}
In this paper, we present a thorough experience on tuning single precision general matrix multiplication(SGEMM)
in assembly level on Kepler GK110 architecture. First, we propose a methodology to demistify GPU's
microarchitecture-level optimizations. Based on the our own assembler KeplerAs, we debunk the
mystery of three performance-critical factors of microarchitecture,
which are control code pattern, register bank mapping and instruction throughput. Second, we tune
SGEMM performance by applying a comprehensive optimization which is never done in either CUDA or
PTX. The optimization strategies go through the architecture hierarchy including maximizing FFMA
throughput (core), eliminating register bank conflict (register), reducing global memory
transactions (shared memory), selecting data path to global memory (off-chip memory).
The optimized SGEMM TN, NN achieve an maximal floating-point efficiency of $88\%$ and $83\%$, which are $25\%$ and
$12\%$ higher than Cublas's $63\%$ and $71\%$ respectively.
This work will facilitate performance tuning of other core kernels on GPUs.
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% We no longer use \terms command
%\terms{Theory}

\keywords{SGEMM; Assembler; GPU; Performance}

\section{Introduction}
Single Precision General Matrix Multiply (SGEMM) performs a multiplication of two single-precision matrices and is implemented as a basic routine of Level-3 in BLAS library~\cite{}. It has been extensively used in many scientific and engineering computing applications. Recently, SGEMM has drawn more and more effort on performance tuning since it is the performance critical kernel in deep learning applications~\cite{}.

As GPU provides more peak FLOP/S over CPU contemporarily, people tend to adopt GPUs to accelerate their floating-points intensive applications. In fact, SGEMM's performance highly relies on low level microarchitecture features. Hardware vendors also provide BLAS libraries tuned on their own processors, i.e. MKL/ACML for multicore x86 CPUs, CUBLAS/CLMath for GPUs. However, we always witness improvement of the third-party tuned implementations over these vendors' libraries. For multicore CPUs, based on the hand tuned assembly codes, OpenBLAS~\cite{} achieves the best performance in most cases. Although there is lack of an OpenBLAS-like library on GPUs, we observe that several on-going efforts~\cite{} achieve better performance than CUBLAS for either SGEMM or DGEMM by tuning assembly codes, too. Figure~\ref{} compares the maximal floating-point efficiencies achieved by SGEMMs in CUBLAS and third-party implementations. The third-parties improve performance by $xx\%$, $xx\%$ and $xx\%$ on G80, Fermi and Maxwell CUDA architectures, respectively. Unfortunately, there is no such a performance tuning work for SGEMM on Kepler architecture, which is the most popular architecture in current Tesla GPUs market. This work attempts to explore improvement space of floating-point efficiency of SGEMM on Kepler GPU.

However, there remain two issues to be addressed for accomplishing the microarchitecture based performance tuning on each generation of GPUs.
\begin{itemize}
\item {\em There is lack of a toolchain to identify GPU microarchitecture features and guide performance tuning.} Unlike general-purpose CPU community where a series of toolchains are available to tune performance in a bare metal way, only is the abstract model CUDA is encouraged. A major reason is the significant changes of each generation of GPU architecture. For example, the recent ISA is totally different from that of the first generation of GPU. The ISA is evolving to the trend where the graphics features are becoming less and less. Other microarchitectural details like register bank conflict and floating-point instructions dual issue are also different between two generations of GPUs. Fortunately, people have made some initial progress on such performance tuning tools due to the pursuit of extreme performance. These efforts are involved with either benchmarking~\cite{} or disassembler~\cite{} on a specific GPU architecture. We attempt to develop a methodology that provides a systematic way to identify microarchitecture by benchmarking, automatically decode instruction formats and generate ISA-compatible executable codes.

\item {\em There is lack of a comprehensive understanding of SGEMM's performance in terms of low level GPU microarchitecture.} Due to the lack of bare-metal tools on GPUs, most of SGEMM analysis are circumscribed at the levels of either CUDA  or PTX. Unfortunately, these analysis cannot directly diagnose neither compiler deficiency nor hardware defect. In fact, by observing the disassembled code of SGEMM in CUDA, we find that the generated control code is
very inefficient in exploiting FFMA dual issue. This means NVCC generated code does not
utilize shared $64$ cores on $192$ core SM, leaving them idle at most of time. A side effect of the limitation leads to a bias estimation of performance bound. We present a thorough analysis of performance which goes through the whole architecture hierarchy including instruction throughput, register allocation, shared memory transactions and global memory. The understanding of microarchitecture optimization help build a robust performance model of SGEMM.


\end{itemize}

In this paper we crack instruction encoding by performing a tough reverse engineering work, and build assembler to generate CUDA binary file for ISA-compatible assembly codes. Basically, due to the compatible grammar with CUDA {\tt cuobjdump}, we use CUDA toolchains to compile CUDA codes to {\tt cubin} file and then disassemble it to generate assembly codes. This approach supports users to optimize any code segment on the base of generated code instead of coding from scratch. By using this assembler, we design a set of microbenchmarks that demystify lots of GPU microarchitecture details such as instruction issue, warp schedule, register bank distribution and control code. It's helpful to understand and optimize performance of SGEMM. We apply a collection of optimizations to incrementally improve SGEMM performance one by one. More specifically, we make the following contributions:
\begin{itemize}
\item We propose an approach to reverse engineering approach to crack instruction encoding of GPU architecture, especially for control codes that orchestrate instruction scheduling. An assembler is developed to directly tune the assembly codes generated by CUDA compiler.
\item We design a bunch of benchmarks to reveal microarchitectural features which are undocumented by NVIDIA. They are important to understand and tune performance of GPU programs.
\item We implement the highest performance SGEMM routines on NVIDIA Kepler by applying the demystified microarchitecture-level optimizations. The achieved maximal floating-point efficiency is xx and xx on two GPUs, respectively.
\end{itemize}

Note that this work demonstrates the effectiveness on NVIDIA Kepler architecture. However, the approach is general for other NVDIA GPU architectures by minor  adjustments of instruction solver and benchmarking implementation. The demystified microarchitecture-specific optimizations are further applied performance tuning on other NVIDIA GPU architectures. The rest of this paper is organized as follows.
\section{Background}
Since we seek to establish an assembly-level optimization approach to SGEMM, this section introduces some CUDA binary utilities which are used in our work. Besides, we highlight the tunable factors that determine SGEMM performance on GPU architecture.

\subsection{CUDA Binary Utilities}
\label{sec:cuda}

A CUDA binary file is an ELF-formatted file generated by CUDA compiler {\tt nvcc}. It is also called {\em cubin} file, which can be either embedded into the host executable file or generated separately by using the "{\em -cubin}" option of {\tt nvcc}. In order to check {\em cubin} files, CUDA toolkit introduces three binary tools and documents on ISAs of several generations of GPUs from GT200 to Maxwell architecture. These binary tools only provide very limited functionalities to users. For instance, both {\tt cuobjdump} and {\tt nvdiasm} disassemble {\em cubin} files to {\em sass} ones, which are readable assembly codes for examining possible performance issues in GPU programs. For each instruction in the executable code sections, the tools list its address in hexadecimal, name in string and encoded number in hexadecimal. For example, an {\tt IADD} instruction might be printed as follows: \\\\
$/*0048*/~~~~IADD~~R0,~~R2,~~R0;~~~~/* 0x4800000000201c03 */$\\\\
Unfortunately, these tools do not enable us to directly modify the assembly codes for tuning performance. What we can do is to refine CUDA C codes after examining the read-only assembly codes iteratively. In fact, an assembler is required to manipulate assembly codes. Although NVIDIA doesn't release its internal assembler and instruction encoding format, the printed third field--encoded number, combining with the released ISA reference~\cite{}, provides clue to crack instruction format (in Section~\ref{sec:assembler}).


\subsection{Performance Factors of SGEMM }
It's well-known that the primary factors of SGEMM performance are the blocking parameters for exploiting data reuse through memory hierarchy. For an instance of GPU, they are shared memory blocking factor and register blocking factor. For the purpose of completeness, we describe a blocking algorithm which is similar with that in other literatures~\cite{magma}~\cite{nervana_sgemm_wiki}~\cite{lai}~\cite{tan}.

Algorithm~\ref{gemm} shows the skeleton of SGEMM blocking algorithm. Task partition is based on result matrix $C$.
Each thread block of $tx*ty$ threads is responsible to compute a $bm*bn$ block of submatrix $C$,
where $bm, bn$ is number of rows and columns of submatrix $C$, respectively.
In order to compute $bm*bn$ block of submatrix $C$, we need $bm*bk$ submatrix from A and $bk*bn$
sub-matrix from B.
In this way, $A$, $B$ and $C$ are divided into $M*K$, $K*N$ and $M*N$ grids of $bm*bk$, $bk*bn$ and $bm*bn$ blocks, respectively, where
$M=\Bigl\lfloor \frac{m+bm-1}{bm} \Bigr\rfloor$,
$K=\Bigl\lfloor \frac{k+bk-1}{bk} \Bigr\rfloor$,
$N=\Bigl\lfloor \frac{n+bn-1}{bn} \Bigr\rfloor$.

\begin{algorithm}
      \caption{SGEMM blocking algorithm}\label{gemm}
  \begin{algorithmic}[1]
      \State The size of thread block: $tx*ty$
      \State Register: accum[$rx*ry$], rA[$2*rx$], rB[$2*ry$]
      \State load one $bm*bk$ block of A into smA[$bk$][$bm$]
      \State load one $bk*bn$ block of B into smB[$bk$][$bn$]
      \Do
      \For {i = 1 to $bk$ } \Comment Unrolling for loop
      \State load one column of A in smA into rA[0 . . . $rx$]
      \State load one row of B in smB into rB[0 . . . $ry$]
      \State accum[0 . . . $rx$][0 . . .$ry$]+ = rA[0 . . . $rx$]* rB[0 . . . $ry$]
      \EndFor
      \State load one $bm*bk$ block of A into smA[$bk$][$bm$]
      \State load one $bk*bn$ block of B into smB[$bk$][$bn$]
      \State bar.sync
      \doWhile {pointer in B is out of range}
      \State merge accum[0 . . . $rx$][0 . . . $ry$] with $bm*bn$ block of C.
  \end{algorithmic}
\end{algorithm}

According to the blocking algorithm, SGEMM performance is affected by the hierachical memory blocking and unrolling factors in the inner loops. Table~\ref{tab:reg} estimates data movement volume through register, shared memory and global memory. The tuning of these factors is conducted in two folds which are memory bandwidth and latency. In fact, with the estimations in Table~\ref{tab:reg} it is relatively easy to tune proper parameters to eliminate bound of memory bandwidth~\cite{magma}~\cite{tan}. The difficulty is latency, which is closely related to specific microarchitecture such as instruction sequence, instruction type and so on. However, the microarchitectural optimizations depends on the availability of an assembler and deep understanding of microarchitectural features.

\begin{table}[!t]
\caption{Data movement volume in the blocking algorithm}
\centering
\scalebox{1.0} {
\begin{tabular}{|c||c|}
\hline
Data Path& Data Volume\\
\hline
global2reg (LDG)& $bm*bk/rx + bn*bk/ry$\\
\hline
reg2shared (STS)& $bm*bk/rx + bn*bk/ry$\\
\hline
shared2reg (LDS)& $bm*bk + bn*bk$\\
\hline
\end{tabular}
}
\label{tab:reg}
\end{table}

\section{Demystifying Microarchitecture Feature}
\label{sec:assembler}

\subsection{Methodology}
We propose a methodology to demystify GPU microarchitecture features and correlate them with performance. The workflow consists of three components.  The first one is CUDA binary tools, which are leveraged to generate assembly codes for sample programs or libraries. A sample program is synthetic CUDA file which is targeted to generate some specific instructions. A library might provide a high coverage of instruction sets. For example, the CUBLAS library contains almost all instructions used in SGEMM routine. As introduced in section~\ref{sec:cuda} these generated assembly files ({\em sass}) provide instruction encoded number to be cracked.

The instruction solver takes the assembly files as input to decode 64-bits binary representation of each instruction. We design a set of algorithms to solve all fields of the binary instruction. These fields include {\em register, predicate, address, immediate, constant} and {\em opcode}. The solver retrieves the undocumented ISA specification, which is used to implement an native assembler. Then, we design a rigorous microbenchmark and leverage the assembler to tune code at assembly language level. In the end, the tuning process will lead to some practical observations on the correlation between microarchitecture and performance.

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.45]{methodology}
\caption{A schematic diagram of demystifying GPU microarchitecture features by leveraging CUDA binary tools. The back arrows represents the workflow of instruction solver while the red ones represents that of benchmarking to find out correlation between microarchitecture and performance.}
\label{fig:workflow}
\end{center}
\end{figure}

\subsection{Instruction Solver}

\subsubsection{Fixed Length Field}
\subsubsection{Variable Length Opcode}
\subsubsection{Modifier}
%LD.E

\subsection{Correlating Microarchitecture with Performance}
With the assembler we tune the assembly codes of microbenchmark. According to the performance tuning results, we correlate microarchitecture with performance variants that guide performance optimizations in real applications. The correlation is presented as several meaningful observations, which are categorized into four microarchitectural features of {\tt control} function, {\tt register} allocation, {\tt arithmetic} throughput and {\tt memory} operation.

\begin{figure}[htbp]
\begin{center}
%\includegraphics[scale=0.7]{ctrl}
\caption{Different control codes regulate {\tt FFMA} throughput.}
\label{fig:control}
\end{center}
\end{figure}

{\em {\bf Observation 1--[Control]}: The execution sequence of instructions is regulated by control codes. Both warp scheduling and issue mode are tunable by setting control codes.}

Starting with the Kepler architecture NVIDIA has been moving some control logic off of the chip and into kernel instructions which are determined by the assembler. This evolution provides programmer a chance to make globally optimal decisions on scheduling and other control aspects if an assembler is available. The disassembly code indicates that every 64 bits control code controls $7$ instructions. We identify that both higher $6$ bits and lower 2 bits are {\em opcode} of control code, and the middle 56 bits are used to control the execution of $7$ instructions, each of which is assigned $8$ control bits.

For the eight control bits, we identify their meanings by examining CUBLAS disassembly codes and tuning a {\tt FFMA} throughput microbhenchmark. We observe that the $7$th bit of the control code of {\tt TLD} instruction is $1$, which indicates a texture dependency barrier due to weak consistent memory model. It's verified that some illegal values are loaded if this bit is not set. Similarly, we discover that the $5$th bit means shared memory dependency barrier, the $4$th bit means global memory dependency barrier. Figure~\ref{} shows that {\tt FFMA} throughput varies with all control bit values from 0 to 255. As shown in this figure, the throughput linearly decreases with the increasing values represented by the $0-3$ bits. That implies that these $4$ bits set the number of stall cycles before issuing the instruction. Further, the microbenchmarking reveal some specific patterns of control codes:

\begin{itemize}
\item When the control bits are set to be $0x40$, the scheduler suspends a warp of the instructions for 32 cycles.
\item $0x04$ means dual issue mode. If two consecutive instructions is controlled by $0x04$ and $0x05$, the throughput can reach the maximum. Single issue control code is $0x00$.
\item $0x20|n$ means a warp is suspended for $n$ cycles before issuing the next instruction, where $n$ is number between 0 and 15.
\end{itemize}


{\em {\bf Observation 2--[Register]}: Irrespective of single- or dual-issue mode, register bank conflict is only caused by source operands, and degrades instruction throughput by up to $17\%$.}

For CUDA programming model it is well-known that shared memory bank conflict is an important performance factor. In fact, recent researches~\cite{} noticed that register bank conflicts are nontrivial to performance.  In order to probe register bank conflict, our microbenchmark measures instruction throughput for different combination of {\tt FFMA} register operands. Table~\ref{tab:th} shows an example of the combination which results in variance of efficiency. The numbers in the fifth column represent the number of registers conflict in the same bank. This experiment is conducted in single-issue mode by setting control code to be $0x20$. The theoretical efficiency is $128/192=66.67\%$. In fact, we observe that both single- and dual-issue mode produce the same variance of instruction throughput. Besides, from the experimental results we observe that:
\begin{itemize}
\item Destination operand will not contribute to bank conflict, no matter which bank is assigned to it.
\item When source operands have 2-way conflict, the throughput will drop by 2.33\% in single issue
    mode. When source operands have 3-way conflicts, the throughput will drop by 17.17\%.

 \item On Kepler architecture, our microbenchmark finds out a proper distribution of registers for eliminating bank conflict. The distribution is summarized in Table~\ref{tab:reg}, which confirms the follow the rule~\cite{}: \\
 bank0$\Leftarrow$($Rindex \% 8 < 4$ \&\& $Rindex \% 2 == 0$) \\
 bank2$\Leftarrow$($Rindex \% 8 < 4$ \&\&
$Rindex \% 2 == 1$) \\
bank1$\Leftarrow$($Rindex \% 8 > 4$ \&\& $Rindex \%2 == 0$) \\
bank3$\Leftarrow$($Rindex \% 8 < 4$ \&\&
$Rindex\% 2 == 1$)\\
where $Rindex$ is the register number. This rule will guide the performance tuning in the following SGEMM implementation.

\end{itemize}

\begin{table}[htbp]
\caption{The efficiency of instruction throughput varies with difference register bank distribution. {\it Inst} : instruction pattern, {\it Th/SM}: the instruction throughput per SM, {\it Eff}: efficiency of throughput.}
\centering
\scalebox{1.0} {
\begin{tabular}{|c||c|c|c|}
\hline
Inst &Th/SM&Eff&Conflicts \\
\hline
{\tt FFMA R5,R4,R1,R0}&127.50&66.40\%&0\\
\hline
{\tt FFMA R2,R4,R1,R0}&127.50&66.40\%&0\\
\hline
{\tt FFMA R5,R2,R1,R0}&119.18&62.07\%&2\\
\hline
{\tt FFMA R3,R2,R1,R0}&119.18&62.07\%&2\\
\hline
{\tt FFMA R5,R9,R3,R1}&94.52&49.23\%&3\\
\hline
{\tt FFMA R11,R9,R3,R1}&94.52&49.23\%&3\\
\hline
{\tt FMUL R4,R1,R0}&127.50&66.40\%&0\\
\hline
{\tt FMUL R4,R2,R0}&119.17&62.06\%&2\\
\hline
\end{tabular}
}
\label{tab:th}
\end{table}


\begin{table}[htbp]
\caption{Register distribution for zero bank conflict.}
\centering
\scalebox{1.0} {
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|}
\hline
Bank0&0&2&8&10&16&18&24&26&... \\
\hline
Bank1&1&3&9&11&17&19&25&27&... \\
\hline
Bank2&4&6&12&14&20&22&28&30&... \\
\hline
Bank3&5&7&13&15&21&23&29&31&...\\
\hline
\end{tabular}
}
\label{tab:reg}
\end{table}

{\em {\bf Observation 3--[Arithmetic]}: With a proper control code and register allocation, {\tt FFMA} instruction throughput can approach the theoretical peak in dual issue mode.}

It's very intricate to tune instruction execution to improve instruction throughput. The previous work report a maximal throughput of {\tt FFMA} on a SM is $132$, which is much less than the theoretical throughput $192$ on Kepler. Our microbenchmarks reveal several key points of optimization to approach theoretical peak on Kepler. First, the control code must be set properly to dual issue adjacent instructions. Second, the ratio and interval of dual issue {\tt FFMA} instructions must be tuned into a specific pattern. Since each warp of extra computing unit is shared among two warps, when all threads are trying to fully dual issue every two adjacent {\tt FFMA}s, half of the scheduler would stall due to computing resource conflict. The ratio of dual issue and single issue should be $2:2$, and with a proper phase shift among two warp's executing pace, they could get access to the shared computing unit in turn. Third, the first instruction of the core loop needs to be aligned. This restriction is caused by the aligned position of control code in the instruction sequence. Last, {\tt FFMA} dual issue requires 6 register banks. Instruction order has to be adjusted to fully use Kepler's operand collector mechanism to avoid register bank conflicts. As shown in Table~\ref{tab:ffma}, these optimizations together improve {\tt FFMA}'s throughput  to be $190$, which is very close to the theoretical peak $192$.

\begin{table}[htbp]
\caption{Floating-point instruction throughput on Kepler}
\centering
\scalebox{1.} {
\begin{tabular}{|c||c|c|c|}
\hline
Inst name&operation&single issue&dual issue\\
\hline
FFMA&c=a*b+c&127.52&190.35 \\
\hline
FMUL&c=a*b&127.52&190.35 \\
\hline
FADD&c=a+b&127.52&192\\
\hline
\end{tabular}
}
\label{tab:ffma}
\end{table}


{\em {\bf Observation 4--[Memory]}: $128$ bit shared memory load is not as effient as $64$ bit load in bandwith, latency
and impact on FFMA throughput}

We test bandwith, because SGEMM has a minimum requrement of shared memory bandwith as we will talk later. We test latency
because we will use it in instruction reordering. We test impact on other instructions, because we hope {\tt LDS} will not
reduce {\tt FFMA} throughput.
In CUDA document, it is said that {\tt LDS.128} requires $2$ shared memory transactions.
Programer may think the wider bit width of {\tt LDS}, the higher bandwith will achieve. In contract, on Kepler $64$ bit
$LDS$ wins. We benchmark bandwith of {\tt LDS} when no bank conflict occurs in a warp using {\tt LDS.32}, {\tt LDS.64}
and {\tt LDS.128}.
The theoretical shared memory bandwith peak for each SM can be calculated as $Bandwidth=f_{core}*Width*Warpsize$ in
bytes, in which $f_{core}$ means frequency of CUDA core, $Width$ is bank width, Warpsize is warp size. This formular
means if no bank conflict among a warp, then shared memory can be loaded at same clock cycle. On Kepler,
$f_{core}=706HZ$, $Width$ can be $4$ or $8$ due to two bank width configure, $Warpsize$ is $32$, hence the peak bandwith is $188GB/s$.
Table~\ref{} is the result of sequential memory access bankwidth when no bank conflict occurs. This table shows that
{\tt LDS.64} is best of all when timing stream data.
Latency of each instruction is shown in Table~\ref{}, {\tt LDS.128} is longer than {\tt LDS.64}, latency of {\tt LDS.32}
and {\tt LDS.64} is the same.
We test the impact on {\tt FFMA} throughput, two {\tt LDS.64} will have 1.80\% influce, one {\tt LDS.128} instruction has
3.63\% influence on FFMA throughput.
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{lds_bandwidth}
    \caption{ Bandwidth of {\tt LDS.32}, {\tt LDS.64} and {\tt LDS.128}}
\label{fig:lds_bw}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{lds_broadcast_bandwidth}
    \caption{ Broadcast bandwidth of {\tt LDS.32}, {\tt LDS.64} and {\tt LDS.128}}
\label{fig:lds_brd_bw}
\end{center}
\end{figure}

\begin{table}[!t]
\caption{Latency}
\centering
\scalebox{1.0} {
\begin{tabular}{|c||c|}
    \hline
    width & latency(cycles) \\
\hline
    LDS.32 & 42\\
\hline
    LDS.64 & 42\\
\hline
    LDS.128 & 55\\
\hline
\end{tabular}
}
\label{tab:reg}
\end{table}
% global memory
We benchmarked bandwidth and latency of {\tt LD}  and {\tt LDG}. Unlike shared memory, which is SM independent, globlal
memory is Card wise. Since six $64-bit$ memory controlers is distributed on
GK110 and a block of thread only resides on a SM at a time, so blocks of threads must be abundant in order to make full use of six memory controlers.
{\tt LDG} instruction is a global memory load that uses the texture path.
We have $26$ blocks of threads and $512$ threads each block when testing bandwith. Each thread access $4$ words, then
with a stride of $4*blockDim.x*gridDim.x$. We write basic logic in CUDA, extract assembly by our assembler, and then
modify according to different load instructions. $32$, $64$ and $128$ load instruction can be generated by using vector
type, for example int4. We find CUDA compiler can not generate {\tt LDG.E.128} nor {\tt LDG.E.64}. It can only generate
{\tt LD.E}, {\tt LD.E.64}, {\tt LD.E.128}. So we must modify in assembly level in order to test other cases.
{\tt TEXDEPENDBAR} is needed before access data by {\tt LDG} instruction. The bandwith data is shown in Figure~\ref{}.
Global memory bandwith of {\tt LDG} is better than that of {\tt LD}.
We also tested the latency of combination of load width and two load instructions.
Since latency is independent of number of threads, we set thread to be (1,1,1) and block(1,1,1).
In order to create data dependency, we let the value of $ith$ position, to be next position to access. That is, we set
dataIn[i] = (i+stride)\%N. In kernel, we  repeate $256$ times, (j=my\_array[j]), calculate the average latency of $256$ instructions.
This also requires working on assemlby code, because NVCC is not always generate all the cases.

\section{Applying Optimizations to SGEMM }
\label{sec:optimization}
The demystified GPU microarchitecture features provides additional space to tune performance-critical kernels. We apply a series of incremental optimizations to improve SGEMM efficiency on Kepler architecture. The optimization strategies go through architectural hierarchy from core and register to memory. All the optimization strategies are inspired by our microbenchmark's observations.
\begin{itemize}
\item At core level, we orchestrate {\tt FFMA} instruction executions by a more efficient instruction scheduling pattern with respect to the proper control code.
\item At register level, we meticulously map operands to registers so that  bank conflicts are avoided in the inner loop iteration.
\item At memory level, we select appropriate shared memory load/store width and global memory data path to mitigate latencies.
\end{itemize}

\subsection{Instruction Scheduling}
It's ideal to keep warp scheduler dual issue instructions (i.e., {\tt FFMA}) all the time. However, for each SM on Kepler architecture, 64 cores share $4$ schedulers, each of which issues instructions to 32 cores as a warp. As noted in {\em observation 3}, the best pattern of {\tt FFMA} instructions block is a sequence of 2 dual issues (4 {\tt FFMA}s) and 2 single issues ((2 {\tt FFMA}s)). As shown in Figure~\ref{}, the instructions in lines 2-3 and lines 6-7 are respectively grouped as two dual-issues. The other two instructions in line 10 and line 13 are two single issues in terms of floating-point instruction execution. As a comparison, most of the {\tt FFMA}s are single-issues in the CUDA compiler generated codes.


\begin{algorithm}
      \caption{NVCC generated assembly code}\label{nvcc}
  \begin{algorithmic}[1]
      \footnotesize
        \State /*0x08dc109c14801410*/
        \State [
        \State /*0348*/ FFMA R28, R41, R15, R28; /*0xcc007000079ca472*/
        \State /*0350*/ LDS.128 R12, [0x40];     /*0x7a700000201ffc32*/
        \State ]
        \State /*0358*/ FFMA R39, R41, R9, R39; /*0xcc009c00049ca49e*/
        \State /*0360*/ FFMA R29, R41, R10, R29; /*0xcc007400051ca476*/
        \State /*0368*/ FFMA R32, R41, R11, R32; /*0xcc008000059ca482*/
        \State [
        \State /*0370*/ LDS.128 R8, [0x50]; /*0x7a700000281ffc22*/
        \State /*0378*/ IADD R4.CC, R4, R21; /*0xe08400000a9c1012*/
    \State ]
  \end{algorithmic}
\end{algorithm}



\begin{algorithm}
      \caption{The optimized assembly code }\label{nvcc}
  \begin{algorithmic}[1]
      \footnotesize
        \State [
        \State -:-:D:-:04      FFMA R27, R150, R146, R27;
        \State -:-:D:-:05      FFMA R6, R149, R146, R6;
        \State ]
        \State [
        \State -:-:D:-:04      FFMA R7, R148, R147, R7;
        \State -:-:D:-:05      FFMA R2, R149, R147, R2;
        \State ]
        \State [
        \State -:-:D:-:04      FFMA R3, R148, R146, R3;
        \State -:-:-:-:00      LDS.64 R168, [R222+0x400];
        \State ]
        \State -:-:D:-:05      FFMA R9, R148, R152, R9;

        \State ]
  \end{algorithmic}
\end{algorithm}

\begin{figure}[htbp]
\begin{center}
%\includegraphics[scale=0.5]{order}
\caption{{\tt FFMA}s instructions scheduling for calculating a $8\times 8$ subblock of matrix C.}
\label{fig:order}
\end{center}
\end{figure}

Based on the basic {\tt FFMA} instruction block, the scheduling pattern is depicted in Figure~\ref{}, which illustrates the order of 144 {\tt FFMA}s instruction execution for calculating a $12\times 12$ subblock of matrix C. For example, the {\tt FFMA} to calculate $c_{00}$ is the first instruction to be issued. Then, both {\tt FFMA}s to calculate $c_{10}$ and  $c_{11}$ are simultaneously issued. We arrange all the {\tt FFMA} operations according to issue order illustrated in Figure~\ref{}.

Another advantage of this execution order is less register pressure due to register data reuse, which can facilitate operand collector mechanism~\cite{}. Operand collector is storage element coupled from register file and provides inputs to the data path of the processor core for executing an instruction. Operands may be cached and reused in the subsequent instructions. The assembly code in Algorithm~\ref{} lists the instructions to calculate $c_{22}, c_{21}, c_{30}, c_{31}, c_{20}, c_{40}$, where are corresponding to the orders of $7,8,9,10,11,12$ in Figure~\ref{}. The reuse happens as follows. The {\tt FFMA} in line 2 uses the cached operand {\tt R150} from the previous instruction block. Line 3 uses the cached operand {\tt R146} from the previous instruction. Similarly, The {\tt FFMA} in line 7 reuses the cached operands of   {\tt R149} and {\tt R147} from the previous two instructions, respectively.


After setting the order of {\tt FFMA}, other {\tt non-FFMA} instructions should be inserted in proper positions to assure
corretness of the program without losing performance. In order to tolerate instruction latency, we need to keep distance of dependent instructions to be larger than the latency. The distance can be approximately computed as
\begin{displaymath}
distance = \frac{4\times\#instructions}{7}
\end{displaymath}
In a schedule block, there are $7$ instructions which cost $4$ clocks to be issued in dual issue mode. In other word, if we want to have interleave
two instructions of $L$ distance, then $\frac{L*7}{4}$ instructions are needed. Besides, the remained number of slots to insert these instructions is estimated as follows:

\begin{displaymath}
\#slots = \frac{bm\times bn\times unroll}{ffmas\_in\_schedule\_block}=\frac{12\times 12\times 4}{6}=24\times 4
\end{displaymath}

According to these principles, we first arrange {\tt LDS}, {\tt STS}, {\tt LDG} due to their long latencies. The schedule slots are illustrated in two dimension as shown in Table~\ref{}.
Note that we use double buffers to hide latency of {\tt LDG} from global memory, which costs $200$ clock cycles.
Every $4$ loop requires $2$ {\tt LDG}s to load data from global memory to registers, $4$ {\tt STS}s to store data from registers to shared memory. There is a read after write (RAW) dependency between {\tt
LDG} and {\tt STS}. The simple model estimates that $\frac{200\times 7}{4} = 350$ are needed instructions between them. As shown in Table~\ref{}, we put {\tt LDG} and  {\tt STS} in position $P[77][3]$ and position $P[65][2]$, respectively. Thus, the number of instructions between them are $143-77 + 144\times 2 +
65=419$, resulting in a distance of $\frac{4\times 419}{7}=239$ clocks, which is enough to hide latency of {\tt LDG}s.

The arrangement of {\tt LDS}s, which load data from shared memory for double buffers of both $A$ and $B$, follows the same way with {\tt LDG}s.
A {\tt LDS} has a latency of $40$ clock cycles, $\frac{40\times 7}{4}=70$ instructions are needed to interleave {\tt LDS} and {\tt
FFMA}. As shown in Table~\ref{}, {\tt LDS} in $P[11][3]$ read data from {\tt STS} in position $P[65][2]$, the distance is more than $40$ clock
cycles. In the end, a {\tt BAR.SYNC} should be inserted after {\tt STS} before {\tt LDS} to make sure that data in shared memory is ready. Other instructions like {\tt XOR},
{\tt IADD}, {\tt ISETP} are inserted according to data dependency, there is almost no performance loss due to little latency.

\begin{table}[!t]
\caption{The position table of {\tt Non-FFMA} instructions. The inner-loop is unrolled by 4 times. The first column records slot numbers.}
\label{tab:position}
\captionsetup{font=scriptsize}
\centering
\scalebox{0.78} {
\begin{tabular}{|c|c|c|c|c|}
\hline
    \diagbox[width=4em, height=3em, trim=1]{slot}{unroll} & 0 &1 &2 &3 \\
    \hline
    5 & ISET P0 & IADD trackA0 & & XOR readBs \\
    \hline
    11 & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs \\
    \hline
    17 & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs \\
    \hline
    23 & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs \\
    \hline
    29 & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs \\
    \hline
    35& IADD K, -4 & IADD trackA1 & TEXDEPBAR & \\
    \hline
    41 & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs \\
    \hline
    47 & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs \\
    \hline
    53 & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs \\
    \hline
    59 & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs \\
    \hline
    65 & & &STS.64 writeS & ISETP P2 \\
    \hline
    71 & & & & \\
    \hline
    77 & & IADD trackB0 & & LDG trackA \\
    \hline
    83 & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs \\
    \hline
    89 &ISETP P3 & & &\\
    \hline
    95 & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs & LDS.64 readAs \\
    \hline
    101 & & & STS.64 loadB0 & LDG trackB \\
    \hline
    107 & & & STS.64 loadB2 & XOR writeS \\
    \hline
    113 & & & & \\
    \hline
    119 & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs \\
    \hline
    125 & & & XOR readAs & \\
    \hline
    131 & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs & LDS.64 readBs \\
    \hline
    137 & & & & \\
    \hline
    143 & & IADD trackB1 & BAR.SYNC & BAR Loop \\
    \hline
\end{tabular}
}

\end{table}

\subsection{Register Allocation}

To allocate register for $A$ column, $B$ row and $C$ submatrix, we have three considerations: correctness, no bank
conflict and allocate register index tightly.
{\tt LDG.128} restricts $4$ words alignment restriction for register.
Nvidia GPU does not have $128$ bit register, in order to use $128$ bit load, one destination register $RN$ is given, result is writen to
RN+1, RN+2, RN+3. There is undocumented restriction: N\%4==0, otherwise illegal instruction error will be reported.
It's not hard to understand this restriction, $4$ words align for LDG.128 make hardware logic simpler saving die area.
Since we will use $LDG.128$ to load $A$ and $B$, there are $2$ bank allocation choices due to N\%4==0 restriction and
bank distribution of Kepler. We assume allocate $A$ matrix bank $\begin{bmatrix} 0 \\ 1  \end{bmatrix}$,
$B$ bank $\begin{bmatrix} 2 & 3 \end{bmatrix}$ as shown in Figure, we have 2 choices left for $C$.
$\begin{bmatrix} 1 & 2 \\ 3 & 0  \end{bmatrix}$
$\begin{bmatrix} 3 & 1 \\ 0 & 2  \end{bmatrix}$
We have $2i\times2$ bank patterns for SGEMM, we choose $\begin{bmatrix} 0 \\ 1  \end{bmatrix}$ $\begin{bmatrix} 2 & 3 \end{bmatrix}$
$\begin{bmatrix} 1 & 2 \\ 3 & 0  \end{bmatrix}$ for $A$, $B$ and $C$ respectively as shown in Figure.
To allocate actual register index, we choose continuous register index so that register index do not get too big to
exceed 255 restriction. From figure, we can verify that C[i][j], A[i] and B[j] have different banks, for example
C[0][1]'s color is white, A[0]'s color is green and B[1]'s color is red, hence no bank conflicts.
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.45]{reg}
\caption{Register bank allocation: number in the cell is register number. Double buffering column
    $A$ and row $B$. Leftest two columns are registers allocated to one column of $A$ sub-matrix, the
top two rows are registers allocated to one row of sub-matrix $B$, others are registers for
sub-matrix $C$. Green cells means bank0, blue cell means bank1, gray cell means bank2, red cell
means bank3.}
\label{fig:reg}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.45]{order}
\caption{FFMA order: the numbers in the cells mean FFMA executing order, not register number.
Dashed ellipses across two cells mean two FFMA instructions are executed at same clock cycle. }
\label{fig:order}
\end{center}
\end{figure}

\subsection{Memory Movment}
% We recommend abbrvnat bibliography style.
The upper bound of SGEMM bounded by global memory bandwidth can be modeled as
$compuate memory ratio =  bm*bn*bk/(bm*bk*rx + bn*bk*ry)> actural flops/ bandwidth $
We could compute the minimum bandwidth requirement in order to acheive peak $3520$ Gflops.
$3520/x = 192*192*2*4/(2*192*4)=192/1$, the global minimum bandwith requirement is $65$ GB/s.
Similarly, the minimum shared memory requirement
$M/unroll*reg_block*blockdim*gridx*gridy*2*bytes/time = 271GB/s$
For each SM, minimum bandwith requres $20$GB/s.
In choosing which load instruction to use, we consider two aspects: bandwidth,
latency, numbers of instructions requirement and effect on FFMA throughput.
{\tt LDS.64} has fewer impact on {\tt FFMA} throughput and high bandwith, so we choose it in SGEMM optimization.
{\tt LDG.128} has higher bandwith and similar latency with $LDG.64$, we prefer {\tt LDG.128} in our implementation.
\section{Evaluation}
\subsection{Experiment Setup}
\subsection{Overall performance}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{sgemm_tn}
\caption{Cublas vs. our optimized SGEMM }
\label{fig:sgemm_tn}
\end{center}
\end{figure}
\subsection{Performance Analysis}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.65]{tn_prof}
    \caption{Evaluation of each optimization method}
\label{fig:th_prof}
\end{center}
\end{figure}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.6]{block}
    \caption{Evaluation of different blocking size}
\label{fig:block}
\end{center}
\end{figure}


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\end{document}
